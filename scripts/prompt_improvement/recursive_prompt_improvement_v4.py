#!/usr/bin/env python3
"""
ì¬ê·€ì  í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v4
- ìƒì„¸í•œ ê°œë³„ ê²½ì£¼ ë¶„ì„
- í†µí•© ë³µê¸° ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
- í•™ìŠµ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„ 
- prompt-engineering-guide.md ì›ì¹™ ì¤€ìˆ˜
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import time
import re
from collections import defaultdict

class RecursivePromptImproverV4:
    def __init__(self, base_prompt_path: str, max_iterations: int = 5):
        self.base_prompt_path = Path(base_prompt_path)
        self.max_iterations = max_iterations
        self.working_dir = Path("data/recursive_improvement_v4")
        self.working_dir.mkdir(parents=True, exist_ok=True)
        
        self.iteration_history = []
        self.base_version = self._extract_version()
        
    def _extract_version(self) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì¶”ì¶œ"""
        filename = self.base_prompt_path.stem
        match = re.search(r'v(\d+\.\d+)', filename)
        if match:
            return match.group(1)
        return "1.0"  # ê¸°ë³¸ê°’
    
    def get_available_dates(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ì£¼ ë‚ ì§œ ëª©ë¡ ë°˜í™˜"""
        dates = set()
        enriched_files = Path("data/races").glob("*/*/*/*/*_enriched.json")
        
        for file in enriched_files:
            filename = file.name
            match = re.search(r'_(\d{8})_', filename)
            if match:
                dates.add(match.group(1))
        
        return sorted(list(dates))
    
    def count_races_by_date(self, date: str) -> int:
        """íŠ¹ì • ë‚ ì§œì˜ ê²½ì£¼ ê°œìˆ˜ ë°˜í™˜"""
        pattern = f"data/races/*/*/{date}/*/*_enriched.json"
        files = list(Path(".").glob(pattern))
        return len(files)
    
    def run_evaluation_with_date(self, prompt_version: str, prompt_path: str, 
                                date_filter: str, max_workers: int = 3) -> Optional[Dict]:
        """ë‚ ì§œ í•„í„°ë¥¼ ì ìš©í•œ í‰ê°€ ì‹¤í–‰"""
        
        # ì„ì‹œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (ë‚ ì§œ í•„í„° ì ìš©)
        temp_evaluator = self.working_dir / f"evaluate_filtered_{prompt_version}.py"
        self._create_filtered_evaluator(temp_evaluator, date_filter)
        
        cmd = [
            'python3', str(temp_evaluator),
            prompt_version, prompt_path, str(max_workers)
        ]
        
        if date_filter == "all":
            available_dates = self.get_available_dates()
            total_races = sum(self.count_races_by_date(d) for d in available_dates)
            print(f"\ní‰ê°€ ì‹¤í–‰: {prompt_version}")
            print(f"  - ëŒ€ìƒ: ëª¨ë“  ê²½ì£¼")
            print(f"  - ì´ ê²½ì£¼ ìˆ˜: {total_races}ê°œ")
            print(f"  - ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ")
            print(f"  - ì˜ˆìƒ ì‹œê°„: {total_races / max_workers * 5:.0f}ì´ˆ ~ {total_races / max_workers * 10:.0f}ì´ˆ")
        else:
            race_count = self.count_races_by_date(date_filter)
            print(f"\ní‰ê°€ ì‹¤í–‰: {prompt_version}")
            print(f"  - ë‚ ì§œ: {date_filter}")
            print(f"  - ê²½ì£¼ ìˆ˜: {race_count}ê°œ")
            print(f"  - ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ")
        
        print("\ní‰ê°€ ì§„í–‰ ì¤‘...")
        print("-" * 60)
        
        try:
            # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ capture_output=Falseë¡œ ë³€ê²½
            result = subprocess.run(cmd, capture_output=False, text=True)
            
            if result.returncode != 0:
                print(f"Error: í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨ (return code: {result.returncode})")
                return None
            
            # ìµœì‹  í‰ê°€ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            eval_files = list(Path("data/prompt_evaluation").glob(f"evaluation_{prompt_version}_*.json"))
            if not eval_files:
                print("í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
            latest_file = max(eval_files, key=lambda x: x.stat().st_mtime)
            print(f"í‰ê°€ ì™„ë£Œ: {latest_file}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            print(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if temp_evaluator.exists():
                temp_evaluator.unlink()
    
    def analyze_individual_race(self, race_result: Dict, race_data: Optional[Dict] = None) -> Dict:
        """ê°œë³„ ê²½ì£¼ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„"""
        # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if race_result.get("prediction") is None or "predicted" not in race_result:
            return {
                "race_id": race_result["race_id"],
                "error": "No prediction available",
                "error_type": race_result.get("error_type", "unknown")
            }
        
        analysis = {
            "race_id": race_result["race_id"],
            "predicted": race_result["predicted"],
            "actual": race_result["actual"],
            "correct_count": race_result["reward"]["correct_count"],
            "hit_rate": race_result["reward"]["hit_rate"]
        }
        
        # ë§ì¶˜ ë§ê³¼ ë†“ì¹œ ë§ ë¶„ì„
        predicted_set = set(race_result["predicted"])
        actual_set = set(race_result["actual"])
        
        analysis["hits"] = list(predicted_set & actual_set)  # ë§ì¶˜ ë§
        analysis["misses"] = {
            "selected_but_wrong": list(predicted_set - actual_set),  # ì„ íƒí–ˆì§€ë§Œ í‹€ë¦° ë§
            "missed_winners": list(actual_set - predicted_set)       # ë†“ì¹œ ì •ë‹µ ë§
        }
        
        # ë†“ì¹œ ì´ìœ  ë¶„ì„ (enriched ë°ì´í„°ê°€ ìˆë‹¤ë©´)
        if race_data:
            analysis["why_missed"] = self._analyze_miss_reasons(
                analysis["misses"]["missed_winners"], 
                race_data, 
                race_result
            )
            analysis["why_wrong_selection"] = self._analyze_wrong_selections(
                analysis["misses"]["selected_but_wrong"],
                race_data,
                race_result
            )
        
        # ì˜ˆì¸¡ ì „ëµ ë¶„ì„
        if "reasoning" in race_result:
            analysis["strategy_used"] = self._extract_strategy(race_result["reasoning"])
        
        return analysis
    
    def _analyze_miss_reasons(self, missed_horses: List[int], race_data: Dict, race_result: Dict) -> Dict:
        """ë†“ì¹œ ë§ë“¤ì˜ íŠ¹ì„± ë¶„ì„"""
        reasons = {}
        
        # race_dataì—ì„œ ë§ ì •ë³´ ì¶”ì¶œ (êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”)
        horses = race_data.get("horses", [])
        
        for horse_no in missed_horses:
            horse_info = next((h for h in horses if h.get("chulNo") == horse_no), None)
            if horse_info:
                # ë°°ë‹¹ë¥  ìˆœìœ„
                odds_rank = self._get_odds_rank(horse_info, horses)
                
                reason_parts = []
                if odds_rank <= 3:
                    reason_parts.append(f"ì¸ê¸° {odds_rank}ìœ„")
                elif odds_rank >= 8:
                    reason_parts.append(f"ë¹„ì¸ê¸° {odds_rank}ìœ„")
                
                # ê¸°ìˆ˜ ìŠ¹ë¥ 
                if "jkDetail" in horse_info:
                    jk_win_rate = horse_info["jkDetail"].get("winRate", 0)
                    if jk_win_rate > 20:
                        reason_parts.append(f"ê¸°ìˆ˜ìŠ¹ë¥  {jk_win_rate:.1f}%")
                
                reasons[horse_no] = " / ".join(reason_parts) if reason_parts else "ë°ì´í„° ë¶€ì¡±"
        
        return reasons
    
    def _analyze_wrong_selections(self, wrong_horses: List[int], race_data: Dict, race_result: Dict) -> Dict:
        """ì˜ëª» ì„ íƒí•œ ë§ë“¤ì˜ íŠ¹ì„± ë¶„ì„"""
        reasons = {}
        
        horses = race_data.get("horses", [])
        
        for horse_no in wrong_horses:
            horse_info = next((h for h in horses if h.get("chulNo") == horse_no), None)
            if horse_info:
                reason_parts = []
                
                # ê³¼ëŒ€í‰ê°€ ìš”ì¸ ì°¾ê¸°
                odds_rank = self._get_odds_rank(horse_info, horses)
                if odds_rank > 5:
                    reason_parts.append(f"ë°°ë‹¹ë¥  {odds_rank}ìœ„ë¡œ ë¹„ì¸ê¸°")
                
                # ìµœê·¼ ì„±ì 
                if "hrDetail" in horse_info:
                    recent_rank = horse_info["hrDetail"].get("recentAvgRank", 99)
                    if recent_rank > 5:
                        reason_parts.append(f"ìµœê·¼ í‰ê·  {recent_rank:.1f}ìœ„ë¡œ ë¶€ì§„")
                
                reasons[horse_no] = " / ".join(reason_parts) if reason_parts else "ì„ íƒ ê·¼ê±° ë¶ˆëª…"
        
        return reasons
    
    def _get_odds_rank(self, horse: Dict, all_horses: List[Dict]) -> int:
        """ë§ì˜ ë°°ë‹¹ë¥  ìˆœìœ„ ê³„ì‚°"""
        valid_horses = [h for h in all_horses if h.get("winOdds", 0) > 0]
        sorted_horses = sorted(valid_horses, key=lambda x: x["winOdds"])
        
        for idx, h in enumerate(sorted_horses):
            if h.get("chulNo") == horse.get("chulNo"):
                return idx + 1
        return 99
    
    def _extract_strategy(self, reasoning: str) -> str:
        """ì¶”ë¡  ê³¼ì •ì—ì„œ ì „ëµ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì „ëµ ì¶”ì¶œ
        if "ì¸ê¸°ë§ˆ" in reasoning or "ë°°ë‹¹ë¥ " in reasoning:
            return "ë°°ë‹¹ë¥  ì¤‘ì‹¬"
        elif "ê¸°ìˆ˜" in reasoning:
            return "ê¸°ìˆ˜ ì¤‘ì‹¬"
        elif "ìµœê·¼" in reasoning:
            return "ìµœê·¼ ì„±ì  ì¤‘ì‹¬"
        return "ë³µí•© ì „ëµ"
    
    def integrate_iteration_results(self, evaluation_results: Dict, individual_analyses: List[Dict]) -> Dict:
        """ì´í„°ë ˆì´ì…˜ ê²°ê³¼ í†µí•© ë³µê¸°"""
        
        # ì „ì²´ í†µê³„
        total_races = len(individual_analyses)
        total_hits = sum(len(a["hits"]) for a in individual_analyses)
        
        # íŒ¨í„´ ë¶„ì„
        success_patterns = []
        failure_patterns = []
        
        for analysis in individual_analyses:
            if analysis["correct_count"] == 3:  # ì™„ì „ ì„±ê³µ
                success_patterns.append(analysis)
            elif analysis["correct_count"] == 0:  # ì™„ì „ ì‹¤íŒ¨
                failure_patterns.append(analysis)
        
        # ê³µí†µ íŒ¨í„´ ì°¾ê¸°
        missed_horses_stats = defaultdict(int)
        wrong_selection_stats = defaultdict(int)
        
        for analysis in individual_analyses:
            if "why_missed" in analysis:
                for reason in analysis["why_missed"].values():
                    missed_horses_stats[reason] += 1
            if "why_wrong_selection" in analysis:
                for reason in analysis["why_wrong_selection"].values():
                    wrong_selection_stats[reason] += 1
        
        # ê°•ì ê³¼ ì•½ì  ë„ì¶œ
        strengths = []
        weaknesses = []
        
        # ê°•ì  ë¶„ì„
        if evaluation_results["average_correct_horses"] > 1.5:
            strengths.append(f"í‰ê·  {evaluation_results['average_correct_horses']:.1f}ë§ˆë¦¬ ì ì¤‘")
        
        if len(success_patterns) > 0:
            strengths.append(f"{len(success_patterns)}ê°œ ê²½ì£¼ ì™„ì „ ì ì¤‘")
        
        # ì•½ì  ë¶„ì„
        top_missed_reasons = sorted(missed_horses_stats.items(), key=lambda x: x[1], reverse=True)[:3]
        for reason, count in top_missed_reasons:
            if count > total_races * 0.2:  # 20% ì´ìƒì—ì„œ ë°œìƒ
                weaknesses.append(f"{reason} ë§ë“¤ì„ ìì£¼ ë†“ì¹¨ ({count}íšŒ)")
        
        # ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        insights = {
            "strengths": strengths,
            "weaknesses": weaknesses,
            "patterns": {
                "success_rate_by_strategy": self._analyze_strategy_success(individual_analyses),
                "common_miss_patterns": dict(top_missed_reasons[:5]),
                "common_wrong_patterns": dict(sorted(wrong_selection_stats.items(), 
                                                    key=lambda x: x[1], reverse=True)[:5])
            },
            "recommendations": self._generate_recommendations(
                strengths, weaknesses, missed_horses_stats, wrong_selection_stats
            )
        }
        
        return insights
    
    def _analyze_strategy_success(self, analyses: List[Dict]) -> Dict:
        """ì „ëµë³„ ì„±ê³µë¥  ë¶„ì„"""
        strategy_stats = defaultdict(lambda: {"total": 0, "success": 0})
        
        for analysis in analyses:
            if "strategy_used" in analysis:
                strategy = analysis["strategy_used"]
                strategy_stats[strategy]["total"] += 1
                if analysis["correct_count"] >= 2:  # 2ë§ˆë¦¬ ì´ìƒ ë§ì¶¤ì„ ì„±ê³µìœ¼ë¡œ
                    strategy_stats[strategy]["success"] += 1
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rates = {}
        for strategy, stats in strategy_stats.items():
            if stats["total"] > 0:
                success_rates[strategy] = stats["success"] / stats["total"] * 100
        
        return success_rates
    
    def _generate_recommendations(self, strengths: List[str], weaknesses: List[str],
                                 missed_stats: Dict, wrong_stats: Dict) -> List[str]:
        """êµ¬ì²´ì ì¸ ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë†“ì¹œ ë§ íŒ¨í„´ ê¸°ë°˜ ê¶Œê³ 
        if "ì¸ê¸°" in str(missed_stats):
            recommendations.append("ì¸ê¸°ë§ˆ í•„í„°ë§ ê¸°ì¤€ ì™„í™” í•„ìš”")
        
        if any("ê¸°ìˆ˜ìŠ¹ë¥ " in reason for reason in missed_stats):
            recommendations.append("ê¸°ìˆ˜ ìŠ¹ë¥  ê°€ì¤‘ì¹˜ ìƒí–¥ ì¡°ì •")
        
        # ì˜ëª» ì„ íƒí•œ ë§ íŒ¨í„´ ê¸°ë°˜ ê¶Œê³ 
        if any("ë¹„ì¸ê¸°" in str(reason) for reason in wrong_stats):
            recommendations.append("ë¹„ì¸ê¸°ë§ˆ ì„ íƒ ê¸°ì¤€ ê°•í™”")
        
        if any("ë¶€ì§„" in str(reason) for reason in wrong_stats):
            recommendations.append("ìµœê·¼ ì„±ì  ê°€ì¤‘ì¹˜ ìƒí–¥")
        
        # ì „ë°˜ì ì¸ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œê³ 
        if len(weaknesses) > len(strengths):
            recommendations.append("ì „ì²´ì ì¸ ì „ëµ ì¬ê²€í†  í•„ìš”")
        
        return recommendations
    
    def create_improved_prompt(self, current_prompt_path: Path, insights: Dict, 
                             new_version: str, evaluation_results: Dict) -> Path:
        """ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì½ê¸°
        with open(current_prompt_path, 'r', encoding='utf-8') as f:
            current_content = f.read()
        
        # prompt-engineering-guide.md ì›ì¹™ì— ë”°ë¥¸ ìƒˆ í”„ë¡¬í”„íŠ¸ ìƒì„±
        improved_prompt = self._build_improved_prompt(
            current_content, insights, evaluation_results, new_version
        )
        
        # ìƒˆ íŒŒì¼ ì €ì¥
        new_prompt_path = self.working_dir / f"prompt_{new_version}.md"
        with open(new_prompt_path, 'w', encoding='utf-8') as f:
            f.write(improved_prompt)
        
        # prompts í´ë”ì—ë„ ë³µì‚¬
        official_path = Path("prompts") / f"prediction-template-{new_version}.md"
        with open(official_path, 'w', encoding='utf-8') as f:
            f.write(improved_prompt)
        
        print(f"\nê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±:")
        print(f"  - ì‘ì—… ê²½ë¡œ: {new_prompt_path}")
        print(f"  - ê³µì‹ ê²½ë¡œ: {official_path}")
        
        return official_path
    
    def _build_improved_prompt(self, current_content: str, insights: Dict, 
                              evaluation_results: Dict, version: str) -> str:
        """prompt-engineering-guide.md ì›ì¹™ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì„±ê³µ/ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘
        success_examples = self._collect_examples(evaluation_results, "success")
        failure_examples = self._collect_examples(evaluation_results, "failure")
        
        prompt = f"""# ê²½ë§ˆ ì‚¼ë³µì—°ìŠ¹ ì˜ˆì¸¡ í”„ë¡¬í”„íŠ¸ {version}

<context>
í•œêµ­ ê²½ë§ˆ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 1-3ìœ„ì— ë“¤ì–´ì˜¬ 3ë§ˆë¦¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
ì´ì „ ë²„ì „ ì„±ëŠ¥: í‰ê·  ì ì¤‘ {evaluation_results['average_correct_horses']:.1f}ë§ˆë¦¬, ì™„ì „ ì ì¤‘ë¥  {evaluation_results['success_rate']:.1f}%
</context>

<role>
ë‹¹ì‹ ì€ 10ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ í•œêµ­ ê²½ë§ˆ ì˜ˆì¸¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
í†µê³„ì  ë¶„ì„ê³¼ ê²½ë§ˆ ë„ë©”ì¸ ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ì •í™•í•œ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤.
</role>

<task>
ì œê³µëœ ê²½ì£¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 1-3ìœ„ì— ë“¤ì–´ì˜¬ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ 3ë§ˆë¦¬ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”.
</task>

<requirements>
1. ê¸°ê¶Œ/ì œì™¸(win_odds=0) ë§ì€ ë°˜ë“œì‹œ ì œì™¸
2. enriched ë°ì´í„°ì˜ ëª¨ë“  ì •ë³´ í™œìš© (ë§/ê¸°ìˆ˜/ì¡°êµì‚¬ ìƒì„¸ì •ë³´)
3. ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤:
   - ë°°ë‹¹ë¥  (ì‹œì¥ í‰ê°€)
   - ê¸°ìˆ˜ ìŠ¹ë¥  ë° ìµœê·¼ ì„±ì 
   - ë§ì˜ ìµœê·¼ ì…ìƒë¥ 
   - ë¶€ë‹´ì¤‘ëŸ‰ ë³€í™”
   - ê²½ì£¼ ì¡°ê±´ ì í•©ì„±

{self._generate_improvement_rules(insights)}
</requirements>

<analysis_steps>
1. ìœ íš¨í•œ ì¶œì£¼ë§ˆ í™•ì¸ (win_odds > 0)
2. ê° ë§ì˜ í•µì‹¬ ì§€í‘œ ì¶”ì¶œ:
   - ë°°ë‹¹ë¥  ìˆœìœ„
   - ê¸°ìˆ˜ ìŠ¹ë¥  (jkDetail.winRate)
   - ë§ ì…ìƒë¥  (hrDetail.placeRate)
   - ìµœê·¼ ì„±ì  íŠ¸ë Œë“œ
3. ë³µí•© ì ìˆ˜ ê³„ì‚°:
   - ë°°ë‹¹ë¥  ì ìˆ˜: 40%
   - ê¸°ìˆ˜ ì„±ì : 30%
   - ë§ ì„±ì : 30%
4. ìƒìœ„ 3ë§ˆë¦¬ ì„ ì •
5. ì„ ì • ê·¼ê±° ê²€ì¦
</analysis_steps>

<output_format>
```json
{{
  "predicted": [ì¶œì „ë²ˆí˜¸1, ì¶œì „ë²ˆí˜¸2, ì¶œì „ë²ˆí˜¸3],
  "confidence": 60-90 ì‚¬ì´ì˜ ì‹ ë¢°ë„,
  "brief_reason": "í•µì‹¬ ì„ ì • ì´ìœ  (í•œê¸€ 20ì ì´ë‚´)"
}}
```
</output_format>

<examples>
{self._format_examples(success_examples, failure_examples)}
</examples>

<important_notes>
- ì¸ê¸°ë§ˆ(1-3ìœ„)ë¥¼ ë¬´ì‹œí•˜ì§€ ë§ˆì„¸ìš”. í†µê³„ì ìœ¼ë¡œ 50% ì´ìƒì´ ì…ìƒí•©ë‹ˆë‹¤.
- ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‹ ë§ˆëŠ” ë°°ë‹¹ë¥ ì„ ë” ì‹ ë¢°í•˜ì„¸ìš”.
- ê·¹ë‹¨ì ì¸ ë¹„ì¸ê¸°ë§ˆ(10ìœ„ ì´í•˜)ëŠ” íŠ¹ë³„í•œ ì´ìœ ê°€ ì—†ë‹¤ë©´ í”¼í•˜ì„¸ìš”.
</important_notes>
"""
        
        return prompt
    
    def _generate_improvement_rules(self, insights: Dict) -> str:
        """ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ ê°œì„  ê·œì¹™ ìƒì„±"""
        rules = []
        
        # ì•½ì  ê¸°ë°˜ ê·œì¹™
        for weakness in insights.get("weaknesses", []):
            if "ì¸ê¸°" in weakness and "ë†“ì¹¨" in weakness:
                rules.append("4. ì¸ê¸° 1-3ìœ„ ë§ì€ íŠ¹ë³„í•œ ê²°ê²© ì‚¬ìœ ê°€ ì—†ëŠ” í•œ í¬í•¨")
            elif "ê¸°ìˆ˜ìŠ¹ë¥ " in weakness:
                rules.append("5. ê¸°ìˆ˜ ìŠ¹ë¥  15% ì´ìƒì¸ ë§ ìš°ì„  ê³ ë ¤")
        
        # ì¶”ì²œì‚¬í•­ ê¸°ë°˜ ê·œì¹™
        for rec in insights.get("recommendations", []):
            if "ë¹„ì¸ê¸°ë§ˆ ì„ íƒ ê¸°ì¤€ ê°•í™”" in rec:
                rules.append("6. ë°°ë‹¹ë¥  8ìœ„ ì´í•˜ëŠ” ëª…í™•í•œ ê°•ì ì´ ìˆì„ ë•Œë§Œ ì„ íƒ")
        
        return "\n".join(rules) if rules else ""
    
    def _collect_examples(self, evaluation_results: Dict, example_type: str) -> List[Dict]:
        """ì„±ê³µ/ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘"""
        examples = []
        
        for race in evaluation_results.get("detailed_results", []):
            # ì˜ˆì¸¡ì´ ì—†ê±°ë‚˜ rewardê°€ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
            if race.get("prediction") is None or "reward" not in race or "correct_count" not in race["reward"]:
                continue
                
            if example_type == "success" and race["reward"]["correct_count"] == 3:
                examples.append({
                    "race_id": race["race_id"],
                    "predicted": race["predicted"],
                    "actual": race["actual"],
                    "reason": race.get("brief_reason", "")
                })
            elif example_type == "failure" and race["reward"]["correct_count"] == 0:
                examples.append({
                    "race_id": race["race_id"],
                    "predicted": race["predicted"],
                    "actual": race["actual"],
                    "reason": race.get("brief_reason", "")
                })
        
        return examples[:2]  # ê°ê° ìµœëŒ€ 2ê°œì”©
    
    def _format_examples(self, success_examples: List[Dict], failure_examples: List[Dict]) -> str:
        """ì˜ˆì‹œ í¬ë§·íŒ…"""
        formatted = "### ì„±ê³µ ì‚¬ë¡€\n"
        
        for ex in success_examples:
            formatted += f"""
ì…ë ¥: [ê²½ì£¼ ë°ì´í„°]
ì¶œë ¥: {{"predicted": {ex['predicted']}, "confidence": 85, "brief_reason": "{ex.get('reason', 'ì¸ê¸°ë§ˆ ì¤‘ì‹¬ ì„ íƒ')}"}}
ê²°ê³¼: âœ… ì •ë‹µ {ex['actual']}
"""
        
        formatted += "\n### ì‹¤íŒ¨ ì‚¬ë¡€ (í”¼í•´ì•¼ í•  íŒ¨í„´)\n"
        
        for ex in failure_examples:
            formatted += f"""
ì…ë ¥: [ê²½ì£¼ ë°ì´í„°]
ì¶œë ¥: {{"predicted": {ex['predicted']}, "confidence": 70, "brief_reason": "{ex.get('reason', 'ê³ ë°°ë‹¹ ë„ì „')}"}}
ê²°ê³¼: âŒ ì •ë‹µ {ex['actual']} (ë¶„ì„: ì¸ê¸°ë§ˆ ë¬´ì‹œ)
"""
        
        return formatted
    
    def _create_filtered_evaluator(self, output_path: Path, date_filter: str):
        """ë‚ ì§œ í•„í„°ê°€ ì ìš©ëœ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        # evaluate_prompt_v3.py ì½ê¸°
        with open("scripts/evaluation/evaluate_prompt_v3.py", 'r', encoding='utf-8') as f:
            content = f.read()
        
        # find_test_races ë©”ì„œë“œ ìˆ˜ì •
        if date_filter != "all":
            # ë‚ ì§œ í•„í„° ì¶”ê°€
            filter_code = f"""
        # ë‚ ì§œ í•„í„° ì ìš©
        enriched_files = [f for f in enriched_files if '/{date_filter}/' in f]
"""
            content = content.replace(
                'enriched_files = sorted(glob.glob(enriched_pattern))',
                f'enriched_files = sorted(glob.glob(enriched_pattern)){filter_code}'
            )
        
        # main í•¨ìˆ˜ ìˆ˜ì • - test_limitë¥¼ Noneìœ¼ë¡œ ì„¤ì •
        new_main = '''def main():
    if len(sys.argv) < 3:
        print("Usage: python evaluate_prompt_filtered.py <prompt_version> <prompt_file> [max_workers]")
        sys.exit(1)
    
    prompt_version = sys.argv[1]
    prompt_file = sys.argv[2]
    max_workers = int(sys.argv[3]) if len(sys.argv) > 3 else 3
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = PromptEvaluatorV3(prompt_version, prompt_file)
    results = evaluator.evaluate_all_parallel(test_limit=None, max_workers=max_workers)


if __name__ == "__main__":
    main()'''
        
        # main í•¨ìˆ˜ ì „ì²´ë¥¼ êµì²´
        content = re.sub(
            r'def main\(\):.*?if __name__ == "__main__":\s*main\(\)',
            new_main,
            content,
            flags=re.DOTALL
        )
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def run_improvement_cycle(self, date_filter: str = "all", max_workers: int = 3):
        """ì „ì²´ ê°œì„  ì‚¬ì´í´ ì‹¤í–‰"""
        current_prompt_path = self.base_prompt_path
        previous_results = None
        
        # ë‚ ì§œ ì •ë³´ ì¶œë ¥
        if date_filter == "all":
            available_dates = self.get_available_dates()
            total_races = sum(self.count_races_by_date(d) for d in available_dates)
            print(f"=== ì¬ê·€ì  í”„ë¡¬í”„íŠ¸ ê°œì„  v4 ì‹œì‘ ===")
            print(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {self.base_prompt_path}")
            print(f"ê¸°ë³¸ ë²„ì „: v{self.base_version}")
            print(f"ìµœëŒ€ ë°˜ë³µ: {self.max_iterations}íšŒ")
            print(f"í…ŒìŠ¤íŠ¸ ê²½ì£¼: ëª¨ë“  ê²½ì£¼ ({total_races}ê°œ)")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: {', '.join(available_dates)}")
        else:
            race_count = self.count_races_by_date(date_filter)
            print(f"=== ì¬ê·€ì  í”„ë¡¬í”„íŠ¸ ê°œì„  v4 ì‹œì‘ ===")
            print(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {self.base_prompt_path}")
            print(f"ê¸°ë³¸ ë²„ì „: v{self.base_version}")
            print(f"ìµœëŒ€ ë°˜ë³µ: {self.max_iterations}íšŒ")
            print(f"í…ŒìŠ¤íŠ¸ ë‚ ì§œ: {date_filter}")
            print(f"í…ŒìŠ¤íŠ¸ ê²½ì£¼: {race_count}ê°œ")
        
        print(f"ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ")
        print("=" * 60)
        
        for iteration in range(self.max_iterations):
            # ë²„ì „ ìƒì„±
            major_version = int(float(self.base_version)) + 1
            version = f"v{major_version}.{iteration}"
            
            print(f"\n\n{'='*60}")
            print(f"Iteration {iteration+1}/{self.max_iterations} - {version}")
            print(f"{'='*60}")
            
            # 1. í˜„ì¬ í”„ë¡¬í”„íŠ¸ í‰ê°€
            evaluation_results = self.run_evaluation_with_date(
                version, 
                str(current_prompt_path), 
                date_filter,
                max_workers
            )
            
            if not evaluation_results:
                print("í‰ê°€ ì‹¤íŒ¨, ì¤‘ë‹¨")
                break
            
            # 2. ê°œë³„ ê²½ì£¼ ìƒì„¸ ë¶„ì„
            print("\nğŸ“Š ê°œë³„ ê²½ì£¼ ë¶„ì„ ì¤‘...")
            individual_analyses = []
            
            for race_result in evaluation_results.get("detailed_results", []):
                # enriched ë°ì´í„° ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
                race_data = self._load_race_data(race_result["race_id"])
                
                analysis = self.analyze_individual_race(race_result, race_data)
                # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ì£¼ëŠ” ë¶„ì„ì—ì„œ ì œì™¸
                if "error" not in analysis:
                    individual_analyses.append(analysis)
            
            # 3. í†µí•© ë³µê¸° ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
            print("\nğŸ” ì´í„°ë ˆì´ì…˜ ë³µê¸° ì¤‘...")
            insights = self.integrate_iteration_results(evaluation_results, individual_analyses)
            
            # 4. ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
            print(f"  - í‰ê°€ ê²½ì£¼ ìˆ˜: {evaluation_results['total_races']}ê°œ")
            print(f"  - ì™„ì „ ì ì¤‘ë¥ : {evaluation_results['success_rate']:.1f}%")
            print(f"  - í‰ê·  ì ì¤‘ ë§: {evaluation_results['average_correct_horses']:.2f}/3")
            
            print(f"\nğŸ’ª ê°•ì :")
            for strength in insights["strengths"][:3]:
                print(f"  - {strength}")
            
            print(f"\nâš ï¸ ì•½ì :")
            for weakness in insights["weaknesses"][:3]:
                print(f"  - {weakness}")
            
            print(f"\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
            for rec in insights["recommendations"][:3]:
                print(f"  - {rec}")
            
            # 5. ì´ë ¥ ì €ì¥
            self.iteration_history.append({
                "iteration": iteration + 1,
                "version": version,
                "prompt_path": str(current_prompt_path),
                "results": evaluation_results,
                "insights": insights,
                "individual_analyses": individual_analyses,
                "date_filter": date_filter
            })
            
            # 6. ëª©í‘œ ë‹¬ì„± í™•ì¸
            if evaluation_results['success_rate'] >= 70:
                print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±! (ì ì¤‘ë¥  70% ì´ìƒ)")
                break
            
            # 7. ê°œì„  ì •ì²´ í™•ì¸
            if previous_results:
                improvement = evaluation_results['success_rate'] - previous_results['success_rate']
                if improvement < 1.0 and iteration >= 2:
                    print(f"\nâš ï¸ ê°œì„  ì •ì²´ (ê°œì„ í­ {improvement:.1f}%p)")
                    print("ì¶©ë¶„í•œ ì‹œë„ í›„ ì •ì²´, ì¢…ë£Œ")
                    break
            
            # 8. ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ê°œì„ 
            if iteration < self.max_iterations - 1:
                print(f"\nğŸ”§ í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘...")
                
                # ìƒˆ ë²„ì „ ìƒì„±
                new_version = f"v{major_version}.{iteration+1}"
                new_prompt_path = self.create_improved_prompt(
                    current_prompt_path,
                    insights,
                    new_version,
                    evaluation_results
                )
                
                current_prompt_path = new_prompt_path
                previous_results = evaluation_results
                
                # ë‹¤ìŒ ë°˜ë³µ ì „ ëŒ€ê¸°
                print(f"\nâ³ 10ì´ˆ í›„ ë‹¤ìŒ ë°˜ë³µ ì‹œì‘...")
                time.sleep(10)
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self.generate_final_report()
    
    def _load_race_data(self, race_id: str) -> Optional[Dict]:
        """ê²½ì£¼ IDë¡œ enriched ë°ì´í„° ë¡œë“œ"""
        try:
            # race_id í˜•ì‹: race_1_20250601_3
            parts = race_id.split('_')
            date = parts[2]
            race_no = parts[3]
            
            # enriched íŒŒì¼ ì°¾ê¸°
            pattern = f"data/races/*/*/*/{date}/*/*{race_id}*enriched.json"
            files = list(Path(".").glob(pattern))
            
            if files:
                with open(files[0], 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # API ì‘ë‹µì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
                    if 'response' in data and 'body' in data['response']:
                        items = data['response']['body']['items']['item']
                        return {"horses": items if isinstance(items, list) else [items]}
            
        except Exception as e:
            print(f"ê²½ì£¼ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ({race_id}): {e}")
        
        return None
    
    def generate_final_report(self):
        """ìµœì¢… ê°œì„  ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.working_dir / f"improvement_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# í”„ë¡¬í”„íŠ¸ ì¬ê·€ ê°œì„  ë³´ê³ ì„œ v4\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {self.base_prompt_path}\n")
            f.write(f"ì´ ë°˜ë³µ: {len(self.iteration_history)}íšŒ\n")
            
            if self.iteration_history:
                f.write(f"í…ŒìŠ¤íŠ¸ ì¡°ê±´: {self.iteration_history[0]['date_filter']}\n\n")
            
            # ì„±ëŠ¥ ë³€í™” í‘œ
            f.write("## ì„±ëŠ¥ ë³€í™” ì¶”ì´\n\n")
            f.write("| Iteration | Version | ê²½ì£¼ ìˆ˜ | ì ì¤‘ë¥  | í‰ê·  ì ì¤‘ | ì£¼ìš” ê°œì„ ì‚¬í•­ |\n")
            f.write("|-----------|---------|---------|--------|----------|---------------|\n")
            
            for history in self.iteration_history:
                results = history["results"]
                insights = history["insights"]
                main_improvement = insights["recommendations"][0] if insights["recommendations"] else "ì—†ìŒ"
                
                f.write(f"| {history['iteration']} | {history['version']} | "
                       f"{results['total_races']} | "
                       f"{results['success_rate']:.1f}% | "
                       f"{results['average_correct_horses']:.2f} | "
                       f"{main_improvement} |\n")
            
            # ìƒì„¸ ì¸ì‚¬ì´íŠ¸
            f.write("\n## ê° ì´í„°ë ˆì´ì…˜ ìƒì„¸ ë¶„ì„\n")
            for history in self.iteration_history:
                f.write(f"\n### {history['version']}\n")
                
                insights = history["insights"]
                f.write("\n**ê°•ì :**\n")
                for strength in insights["strengths"]:
                    f.write(f"- {strength}\n")
                
                f.write("\n**ì•½ì :**\n")
                for weakness in insights["weaknesses"]:
                    f.write(f"- {weakness}\n")
                
                f.write("\n**ì£¼ìš” íŒ¨í„´:**\n")
                if "common_miss_patterns" in insights["patterns"]:
                    for pattern, count in list(insights["patterns"]["common_miss_patterns"].items())[:3]:
                        f.write(f"- {pattern}: {count}íšŒ\n")
            
            # ê²°ë¡ 
            f.write("\n## ê²°ë¡ \n\n")
            
            if self.iteration_history:
                final_results = self.iteration_history[-1]["results"]
                initial_results = self.iteration_history[0]["results"]
                
                f.write(f"### ì„±ëŠ¥ ê°œì„ \n")
                f.write(f"- ì´ˆê¸° ì„±ëŠ¥: {initial_results['success_rate']:.1f}% "
                       f"(í‰ê·  {initial_results['average_correct_horses']:.2f}ë§ˆë¦¬)\n")
                f.write(f"- ìµœì¢… ì„±ëŠ¥: {final_results['success_rate']:.1f}% "
                       f"(í‰ê·  {final_results['average_correct_horses']:.2f}ë§ˆë¦¬)\n")
                f.write(f"- ì „ì²´ ê°œì„ : {final_results['success_rate'] - initial_results['success_rate']:+.1f}%p\n\n")
                
                # ìµœì  ë²„ì „
                best_iteration = max(self.iteration_history, 
                                   key=lambda x: (x["results"]["success_rate"], 
                                                x["results"]["average_correct_horses"]))
                f.write(f"### ìµœê³  ì„±ëŠ¥ ë²„ì „\n")
                f.write(f"- ë²„ì „: {best_iteration['version']}\n")
                f.write(f"- ì ì¤‘ë¥ : {best_iteration['results']['success_rate']:.1f}%\n")
                f.write(f"- í‰ê·  ì ì¤‘: {best_iteration['results']['average_correct_horses']:.2f}ë§ˆë¦¬\n")
                f.write(f"- í”„ë¡¬í”„íŠ¸ ê²½ë¡œ: {best_iteration['prompt_path']}\n")
                
                # í•µì‹¬ êµí›ˆ
                f.write("\n### í•µì‹¬ êµí›ˆ\n")
                all_recommendations = set()
                for history in self.iteration_history:
                    all_recommendations.update(history["insights"]["recommendations"])
                
                for rec in list(all_recommendations)[:5]:
                    f.write(f"- {rec}\n")
        
        print(f"\n\nğŸ“„ ìµœì¢… ë³´ê³ ì„œ ìƒì„±: {report_path}")
        
        # ìµœì  í”„ë¡¬í”„íŠ¸ë¥¼ ê³µì‹ ìœ„ì¹˜ì— ë³µì‚¬
        if self.iteration_history:
            best_iteration = max(self.iteration_history, 
                               key=lambda x: (x["results"]["success_rate"], 
                                            x["results"]["average_correct_horses"]))
            best_prompt = Path(best_iteration['prompt_path'])
            if best_prompt.exists():
                final_prompt = Path("prompts") / f"prediction-template-optimized-v4.md"
                subprocess.run(['cp', str(best_prompt), str(final_prompt)])
                print(f"ğŸ† ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì €ì¥: {final_prompt}")


def main():
    if len(sys.argv) < 2:
        print("Usage: python recursive_prompt_improvement_v4.py <base_prompt_file> [date_filter] [max_iterations] [max_workers]")
        print("\nExamples:")
        print("  ëª¨ë“  ê²½ì£¼: python recursive_prompt_improvement_v4.py prompts/base-prompt.md all 5 3")
        print("  íŠ¹ì • ë‚ ì§œ: python recursive_prompt_improvement_v4.py prompts/base-prompt.md 20250601 5 3")
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ:")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ í‘œì‹œ
        improver = RecursivePromptImproverV4("dummy")
        dates = improver.get_available_dates()
        for date in dates:
            count = improver.count_races_by_date(date)
            print(f"  - {date}: {count}ê°œ ê²½ì£¼")
        
        sys.exit(1)
    
    base_prompt = sys.argv[1]
    date_filter = sys.argv[2] if len(sys.argv) > 2 else "all"
    max_iterations = int(sys.argv[3]) if len(sys.argv) > 3 else 5
    max_workers = int(sys.argv[4]) if len(sys.argv) > 4 else 3
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(base_prompt).exists():
        print(f"Error: í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_prompt}")
        sys.exit(1)
    
    # ì¬ê·€ ê°œì„  ì‹¤í–‰
    improver = RecursivePromptImproverV4(base_prompt, max_iterations)
    improver.run_improvement_cycle(date_filter, max_workers)


if __name__ == "__main__":
    main()