#!/usr/bin/env python3
"""
ì¬ê·€ì  í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v2
- evaluate_prompt_v3.pyì™€ ì™„ë²½ í˜¸í™˜
- enriched ë°ì´í„° ê¸°ë°˜ ë¶„ì„
- ê³ ë„í™”ëœ ê°œì„  ì „ëµ
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import time
import re
from collections import defaultdict

class RecursivePromptImproverV2:
    def __init__(self, base_prompt_path: str, max_iterations: int = 5):
        self.base_prompt_path = Path(base_prompt_path)
        self.max_iterations = max_iterations
        self.working_dir = Path("data/recursive_improvement_v2")
        self.working_dir.mkdir(parents=True, exist_ok=True)
        
        self.iteration_history = []
        self.base_version = self._extract_version()
        
    def _extract_version(self) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì¶”ì¶œ"""
        filename = self.base_prompt_path.stem
        match = re.search(r'v(\d+\.\d+)', filename)
        if match:
            return match.group(1)
        return "10.3"  # ê¸°ë³¸ê°’
        
    def run_evaluation(self, prompt_version: str, prompt_path: str, 
                      test_limit: int = 30, max_workers: int = 3) -> Optional[Dict]:
        """evaluate_prompt_v3.pyë¥¼ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ í‰ê°€"""
        cmd = [
            'python3', 'scripts/evaluation/evaluate_prompt_v3.py',
            prompt_version, prompt_path, str(test_limit), str(max_workers)
        ]
        
        print(f"\ní‰ê°€ ì‹¤í–‰: {prompt_version}")
        print(f"ëª…ë ¹ì–´: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"Error: {result.stderr}")
                return None
            
            # ìµœì‹  í‰ê°€ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            eval_files = list(Path("data/prompt_evaluation").glob(f"evaluation_{prompt_version}_*.json"))
            if not eval_files:
                print("í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
            latest_file = max(eval_files, key=lambda x: x.stat().st_mtime)
            print(f"í‰ê°€ ì™„ë£Œ: {latest_file}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            print(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_results(self, current_results: Dict, previous_results: Dict = None) -> Dict:
        """ê²°ê³¼ ë¶„ì„ ë° ê°œì„  ë°©í–¥ ë„ì¶œ"""
        analysis = {
            "current_performance": {
                "success_rate": current_results["success_rate"],
                "avg_correct": current_results["average_correct_horses"],
                "error_rate": current_results.get("error_stats", {}).get("parse_error", 0) / current_results["total_races"] * 100
            }
        }
        
        # ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
        if previous_results:
            analysis["improvement"] = {
                "success_rate_change": current_results["success_rate"] - previous_results["success_rate"],
                "avg_correct_change": current_results["average_correct_horses"] - previous_results["average_correct_horses"]
            }
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_patterns = self.analyze_failure_patterns_v3(current_results)
        analysis["failure_patterns"] = failure_patterns
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        analysis["suggestions"] = self.generate_improvement_suggestions_v3(failure_patterns, current_results)
        
        return analysis
    
    def analyze_failure_patterns_v3(self, results: Dict) -> Dict:
        """v3 í‰ê°€ ê²°ê³¼ì— ë§ì¶˜ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„"""
        patterns = {
            "low_odds_missed": 0,  # ë‚®ì€ ë°°ë‹¹ë¥  ë§ ë†“ì¹¨
            "high_odds_selected": 0,  # ë†’ì€ ë°°ë‹¹ë¥  ë§ ì„ íƒ
            "perfect_miss": 0,  # 3ë§ˆë¦¬ ëª¨ë‘ ë†“ì¹¨
            "partial_hit_1": 0,  # 1ë§ˆë¦¬ë§Œ ë§ì¶¤
            "partial_hit_2": 0,  # 2ë§ˆë¦¬ë§Œ ë§ì¶¤
            "confidence_mismatch": []  # ì‹ ë¢°ë„ì™€ ì‹¤ì œ ê²°ê³¼ ë¶ˆì¼ì¹˜
        }
        
        for race in results["detailed_results"]:
            correct_count = race["reward"]["correct_count"]
            
            if correct_count == 0:
                patterns["perfect_miss"] += 1
            elif correct_count == 1:
                patterns["partial_hit_1"] += 1
            elif correct_count == 2:
                patterns["partial_hit_2"] += 1
                
            # ì‹ ë¢°ë„ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
            if "confidence" in race:
                confidence = race["confidence"]
                hit_rate = race["reward"]["hit_rate"]
                if abs(confidence - hit_rate) > 30:  # 30% ì´ìƒ ì°¨ì´
                    patterns["confidence_mismatch"].append({
                        "race_id": race["race_id"],
                        "confidence": confidence,
                        "actual_hit_rate": hit_rate
                    })
        
        # ë†“ì¹œ ë§ê³¼ ì„ íƒí•œ ë§ì˜ ë°°ë‹¹ë¥  ë¶„ì„ì€ 
        # enriched ë°ì´í„°ë¥¼ ì½ì–´ì„œ ìˆ˜í–‰í•´ì•¼ í•¨ (ë³„ë„ êµ¬í˜„ í•„ìš”)
        
        return patterns
    
    def generate_improvement_suggestions_v3(self, patterns: Dict, results: Dict) -> List[Dict]:
        """v3ì— ë§ì¶˜ êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ"""
        suggestions = []
        total_races = results["total_races"]
        
        # ì™„ì „ ì‹¤íŒ¨ìœ¨ì´ ë†’ì€ ê²½ìš°
        if patterns["perfect_miss"] > total_races * 0.3:
            suggestions.append({
                "type": "strategy_change",
                "target": "selection_logic",
                "action": "increase_conservative_approach",
                "reason": f"{patterns['perfect_miss']}/{total_races} ê²½ì£¼ì—ì„œ ì™„ì „ ì‹¤íŒ¨",
                "implementation": "ìƒìœ„ 3ê°œ ì¸ê¸°ë§ˆ ê¸°ë³¸ ì„ íƒ í›„ ì¡°ì •"
            })
        
        # ë¶€ë¶„ ì ì¤‘ì´ ë§ì€ ê²½ìš° (ê°œì„  ê°€ëŠ¥ì„± ë†’ìŒ)
        partial_hits = patterns["partial_hit_1"] + patterns["partial_hit_2"]
        if partial_hits > total_races * 0.5:
            suggestions.append({
                "type": "fine_tuning",
                "target": "scoring_weights",
                "action": "optimize_composite_score",
                "reason": f"{partial_hits}/{total_races} ê²½ì£¼ì—ì„œ ë¶€ë¶„ ì ì¤‘",
                "implementation": "ë°°ë‹¹ë¥  ê°€ì¤‘ì¹˜ ìƒí–¥, ê¸°ìˆ˜/ë§ ì„±ì  ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì •"
            })
        
        # ì‹ ë¢°ë„ ë¶ˆì¼ì¹˜ê°€ ì‹¬í•œ ê²½ìš°
        if len(patterns["confidence_mismatch"]) > total_races * 0.2:
            suggestions.append({
                "type": "calibration",
                "target": "confidence_calculation",
                "action": "recalibrate_confidence",
                "reason": f"{len(patterns['confidence_mismatch'])} ê²½ì£¼ì—ì„œ ì‹ ë¢°ë„ ë¶ˆì¼ì¹˜",
                "implementation": "ì‹ ë¢°ë„ ê³„ì‚° ë¡œì§ ê°œì„ "
            })
        
        # í‰ê·  ì ì¤‘ë¥  ê¸°ë°˜ ì œì•ˆ
        avg_correct = results["average_correct_horses"]
        if avg_correct < 1.0:
            suggestions.append({
                "type": "major_revision",
                "target": "entire_strategy",
                "action": "switch_to_market_driven",
                "reason": f"í‰ê·  ì ì¤‘ {avg_correct:.2f}ë§ˆë¦¬ë¡œ ë§¤ìš° ë‚®ìŒ",
                "implementation": "ì‹œì¥ í‰ê°€(ë°°ë‹¹ë¥ ) ì¤‘ì‹¬ ì „ëµìœ¼ë¡œ ì „í™˜"
            })
        elif avg_correct < 1.5:
            suggestions.append({
                "type": "enhancement",
                "target": "data_usage",
                "action": "better_enriched_data_usage",
                "reason": f"í‰ê·  ì ì¤‘ {avg_correct:.2f}ë§ˆë¦¬ë¡œ ê°œì„  í•„ìš”",
                "implementation": "enriched ë°ì´í„°ì˜ ê¸°ìˆ˜ìŠ¹ë¥ , ë§ì…ìƒë¥  í™œìš©ë„ ì¦ê°€"
            })
        
        return suggestions
    
    def apply_improvements(self, current_prompt_path: Path, 
                          suggestions: List[Dict], 
                          new_version: str) -> Path:
        """ê°œì„ ì‚¬í•­ì„ í”„ë¡¬í”„íŠ¸ì— ì ìš©"""
        with open(current_prompt_path, 'r', encoding='utf-8') as f:
            prompt_content = f.read()
        
        # ê°œì„  ë‚´ì—­ ê¸°ë¡
        improvement_log = f"\n## ê°œì„  ë‚´ì—­ ({new_version})\n"
        improvement_log += f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        for suggestion in suggestions:
            improvement_log += f"- **{suggestion['type']}**: {suggestion['reason']}\n"
            improvement_log += f"  - ëŒ€ìƒ: {suggestion['target']}\n"
            improvement_log += f"  - ì•¡ì…˜: {suggestion['action']}\n"
            improvement_log += f"  - êµ¬í˜„: {suggestion['implementation']}\n\n"
            
            # ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
            if suggestion["type"] == "strategy_change" and "conservative" in suggestion["action"]:
                # ë³´ìˆ˜ì  ì ‘ê·¼ ê°•í™”
                prompt_content = self._apply_conservative_strategy(prompt_content)
                
            elif suggestion["type"] == "fine_tuning" and "composite_score" in suggestion["action"]:
                # ë³µí•© ì ìˆ˜ ê°€ì¤‘ì¹˜ ì¡°ì •
                prompt_content = self._adjust_composite_weights(prompt_content)
                
            elif suggestion["type"] == "major_revision" and "market_driven" in suggestion["action"]:
                # ì‹œì¥ ì¤‘ì‹¬ ì „ëµìœ¼ë¡œ ì „í™˜
                prompt_content = self._apply_market_driven_strategy(prompt_content)
        
        # ë²„ì „ ì •ë³´ ì—…ë°ì´íŠ¸
        prompt_content = re.sub(
            r'# ê²½ë§ˆ ì‚¼ë³µì—°ìŠ¹ ì˜ˆì¸¡.*?\n',
            f'# ê²½ë§ˆ ì‚¼ë³µì—°ìŠ¹ ì˜ˆì¸¡ í”„ë¡¬í”„íŠ¸ {new_version}\n',
            prompt_content
        )
        
        # ê°œì„  ë‚´ì—­ì„ í”„ë¡¬í”„íŠ¸ ìƒë‹¨ì— ì¶”ê°€
        prompt_content = prompt_content.split('\n', 1)
        prompt_content = prompt_content[0] + '\n' + improvement_log + prompt_content[1]
        
        # ìƒˆ íŒŒì¼ ì €ì¥
        new_prompt_path = self.working_dir / f"prompt_{new_version}.md"
        with open(new_prompt_path, 'w', encoding='utf-8') as f:
            f.write(prompt_content)
        
        # prompts í´ë”ì—ë„ ë³µì‚¬
        official_path = Path("prompts") / f"prediction-template-{new_version}.md"
        with open(official_path, 'w', encoding='utf-8') as f:
            f.write(prompt_content)
        
        print(f"\nê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±:")
        print(f"  - ì‘ì—… ê²½ë¡œ: {new_prompt_path}")
        print(f"  - ê³µì‹ ê²½ë¡œ: {official_path}")
        
        return official_path
    
    def _apply_conservative_strategy(self, content: str) -> str:
        """ë³´ìˆ˜ì  ì „ëµ ì ìš©"""
        # ê¸°ì¡´ ì „ëµ ë¶€ë¶„ì„ ì°¾ì•„ì„œ êµì²´
        conservative_strategy = """## í•µì‹¬ ì „ëµ: ë³´ìˆ˜ì  ì ‘ê·¼

1. **ê¸°ë³¸ ì„ íƒ**: ë°°ë‹¹ë¥  1-3ìœ„ ë§ì„ ìš°ì„  ì„ íƒ
2. **êµì²´ ì¡°ê±´**: ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•  ë•Œë§Œ 4ìœ„ ì´í•˜ ë§ë¡œ êµì²´
   - ê¸°ìˆ˜ ìŠ¹ë¥ ì´ í˜„ì €íˆ ë†’ìŒ (20% ì´ìƒ)
   - ë§ì˜ ìµœê·¼ ì…ìƒë¥ ì´ 50% ì´ìƒ
   - êµì²´ ëŒ€ìƒ ì¸ê¸°ë§ˆì— ëª…í™•í•œ ì•½ì  ì¡´ì¬
3. **ì•ˆì „ ì¥ì¹˜**: ë¶ˆí™•ì‹¤í•œ ê²½ìš° í•­ìƒ ì¸ê¸°ë§ˆ ì„ íƒ"""
        
        # ì „ëµ ì„¹ì…˜ êµì²´
        content = re.sub(
            r'## í•µì‹¬ ì „ëµ.*?(?=\n##|\n\n##|\Z)',
            conservative_strategy,
            content,
            flags=re.DOTALL
        )
        return content
    
    def _adjust_composite_weights(self, content: str) -> str:
        """ë³µí•© ì ìˆ˜ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        # í˜„ì¬ v10.3ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ì„œ ì¡°ì •
        new_weights = """### ë³µí•© ì ìˆ˜ ê³„ì‚° (ì¡°ì •ë¨)
- ë°°ë‹¹ë¥  ì ìˆ˜: 50% (ìƒí–¥)
- ê¸°ìˆ˜ ìŠ¹ë¥ : 25% (ìœ ì§€)  
- ë§ ì…ìƒë¥ : 25% (í•˜í–¥)"""
        
        content = re.sub(
            r'### ë³µí•© ì ìˆ˜.*?(?=\n###|\n\n###|\Z)',
            new_weights,
            content,
            flags=re.DOTALL
        )
        return content
    
    def _apply_market_driven_strategy(self, content: str) -> str:
        """ì‹œì¥ ì¤‘ì‹¬ ì „ëµ ì ìš©"""
        market_strategy = """## í•µì‹¬ ì „ëµ: ì‹œì¥ í‰ê°€ ì¤‘ì‹¬

### ê¸°ë³¸ ì›ì¹™
ì‹œì¥(ë°°ë‹¹ë¥ )ì´ ê°€ì¥ ì •í™•í•œ ì˜ˆì¸¡ ì§€í‘œì„ì„ ì¸ì •í•˜ê³ , ë°ì´í„°ëŠ” ë³´ì¡° ì§€í‘œë¡œë§Œ í™œìš©

### ì„ íƒ í”„ë¡œì„¸ìŠ¤
1. ë°°ë‹¹ë¥  ìˆœìœ„ 1-5ìœ„ í™•ì¸
2. 1-3ìœ„ëŠ” ìë™ ì„ íƒ
3. 4-5ìœ„ ì¤‘ì—ì„œ ë‹¤ìŒ ì¡°ê±´ ì¶©ì¡± ì‹œ 3ìœ„ì™€ êµì²´:
   - ê¸°ìˆ˜ ìŠ¹ë¥  15% ì´ìƒ
   - ë§ ìµœê·¼ 3ê²½ì£¼ ì¤‘ 1íšŒ ì´ìƒ ì…ìƒ
   - 3ìœ„ ë§ì— ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ë¶€ì§„ ì´ë ¥

### ì˜ˆì™¸ ì²˜ë¦¬
- ë°°ë‹¹ë¥  0 (ê¸°ê¶Œ/ì œì™¸): ì™„ì „ ì œì™¸
- ì‹ ë§ˆ/ë°ì´í„° ë¶€ì¡±: ë°°ë‹¹ë¥ ë§Œìœ¼ë¡œ í‰ê°€"""
        
        # ì „ì²´ ì „ëµ êµì²´
        content = re.sub(
            r'## í•µì‹¬ ì „ëµ.*?(?=\n## ì‘ë‹µ í˜•ì‹|\Z)',
            market_strategy + "\n",
            content,
            flags=re.DOTALL
        )
        return content
    
    def run_improvement_cycle(self, initial_test_limit: int = 30, max_workers: int = 3):
        """ì „ì²´ ê°œì„  ì‚¬ì´í´ ì‹¤í–‰"""
        current_prompt_path = self.base_prompt_path
        previous_results = None
        
        print(f"=== ì¬ê·€ì  í”„ë¡¬í”„íŠ¸ ê°œì„  v2 ì‹œì‘ ===")
        print(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {self.base_prompt_path}")
        print(f"ê¸°ë³¸ ë²„ì „: v{self.base_version}")
        print(f"ìµœëŒ€ ë°˜ë³µ: {self.max_iterations}íšŒ")
        print(f"í…ŒìŠ¤íŠ¸ ê²½ì£¼ ìˆ˜: {initial_test_limit}ê°œ")
        print(f"ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ")
        print("=" * 60)
        
        for iteration in range(self.max_iterations):
            # ë²„ì „ ìƒì„±: v11.0, v11.1, v11.2...
            major_version = int(float(self.base_version)) + 1
            version = f"v{major_version}.{iteration}"
            
            print(f"\n\n{'='*60}")
            print(f"Iteration {iteration+1}/{self.max_iterations} - {version}")
            print(f"{'='*60}")
            
            # 1. í˜„ì¬ í”„ë¡¬í”„íŠ¸ í‰ê°€
            evaluation_results = self.run_evaluation(
                version, 
                str(current_prompt_path), 
                initial_test_limit,
                max_workers
            )
            
            if not evaluation_results:
                print("í‰ê°€ ì‹¤íŒ¨, ì¤‘ë‹¨")
                break
            
            # 2. ê²°ê³¼ ë¶„ì„
            analysis = self.analyze_results(evaluation_results, previous_results)
            
            # 3. ì´ë ¥ ì €ì¥
            self.iteration_history.append({
                "iteration": iteration + 1,
                "version": version,
                "prompt_path": str(current_prompt_path),
                "results": evaluation_results,
                "analysis": analysis
            })
            
            # 4. ì„±ëŠ¥ ì¶œë ¥
            print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
            print(f"  - ì™„ì „ ì ì¤‘ë¥ : {evaluation_results['success_rate']:.1f}%")
            print(f"  - í‰ê·  ì ì¤‘ ë§: {evaluation_results['average_correct_horses']:.2f}/3")
            print(f"  - ì—ëŸ¬ìœ¨: {analysis['current_performance']['error_rate']:.1f}%")
            
            if previous_results:
                print(f"\nğŸ“ˆ ê°œì„  í˜„í™©:")
                print(f"  - ì ì¤‘ë¥  ë³€í™”: {analysis['improvement']['success_rate_change']:+.1f}%p")
                print(f"  - í‰ê·  ì ì¤‘ ë³€í™”: {analysis['improvement']['avg_correct_change']:+.2f}")
            
            # 5. ëª©í‘œ ë‹¬ì„± í™•ì¸
            if evaluation_results['success_rate'] >= 40:
                print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±! (ì ì¤‘ë¥  40% ì´ìƒ)")
                break
            
            # 6. ê°œì„  ì •ì²´ í™•ì¸
            if previous_results:
                if (analysis['improvement']['success_rate_change'] < 1.0 and
                    analysis['improvement']['avg_correct_change'] < 0.1):
                    print(f"\nâš ï¸ ê°œì„  ì •ì²´ ê°ì§€")
                    if iteration >= 2:  # ìµœì†Œ 3íšŒëŠ” ì‹œë„
                        print("ì¶©ë¶„í•œ ì‹œë„ í›„ ì •ì²´, ì¢…ë£Œ")
                        break
            
            # 7. ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ê°œì„ 
            if iteration < self.max_iterations - 1:
                print(f"\nğŸ”§ ê°œì„ ì‚¬í•­ ì ìš©:")
                
                # ê°œì„  ì œì•ˆ ì¶œë ¥
                for i, suggestion in enumerate(analysis["suggestions"][:3]):  # ìƒìœ„ 3ê°œë§Œ
                    print(f"\n{i+1}. {suggestion['type'].upper()}")
                    print(f"   ì´ìœ : {suggestion['reason']}")
                    print(f"   êµ¬í˜„: {suggestion['implementation']}")
                
                # ìƒˆ ë²„ì „ ìƒì„±
                new_version = f"v{major_version}.{iteration+1}"
                new_prompt_path = self.apply_improvements(
                    current_prompt_path,
                    analysis["suggestions"],
                    new_version
                )
                
                current_prompt_path = new_prompt_path
                previous_results = evaluation_results
                
                # ë‹¤ìŒ ë°˜ë³µ ì „ ëŒ€ê¸°
                print(f"\nâ³ 10ì´ˆ í›„ ë‹¤ìŒ ë°˜ë³µ ì‹œì‘...")
                time.sleep(10)
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self.generate_final_report()
    
    def generate_final_report(self):
        """ìµœì¢… ê°œì„  ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.working_dir / f"improvement_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# í”„ë¡¬í”„íŠ¸ ì¬ê·€ ê°œì„  ë³´ê³ ì„œ v2\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {self.base_prompt_path}\n")
            f.write(f"ì´ ë°˜ë³µ: {len(self.iteration_history)}íšŒ\n\n")
            
            # ì„±ëŠ¥ ë³€í™” í‘œ
            f.write("## ì„±ëŠ¥ ë³€í™” ì¶”ì´\n\n")
            f.write("| Iteration | Version | ì ì¤‘ë¥  | í‰ê·  ì ì¤‘ | ì—ëŸ¬ìœ¨ | ë³€í™” |\n")
            f.write("|-----------|---------|--------|----------|--------|------|\n")
            
            for i, history in enumerate(self.iteration_history):
                results = history["results"]
                analysis = history["analysis"]
                change = ""
                if i > 0:
                    prev_rate = self.iteration_history[i-1]["results"]["success_rate"]
                    change = f"{results['success_rate'] - prev_rate:+.1f}%p"
                
                error_rate = analysis["current_performance"]["error_rate"]
                
                f.write(f"| {history['iteration']} | {history['version']} | "
                       f"{results['success_rate']:.1f}% | "
                       f"{results['average_correct_horses']:.2f} | "
                       f"{error_rate:.1f}% | {change} |\n")
            
            # ì£¼ìš” ê°œì„ ì‚¬í•­
            f.write("\n## ì£¼ìš” ê°œì„ ì‚¬í•­\n")
            for history in self.iteration_history:
                if history["analysis"].get("suggestions"):
                    f.write(f"\n### {history['version']}\n")
                    for suggestion in history["analysis"]["suggestions"][:3]:
                        f.write(f"\n**{suggestion['type']}**: {suggestion['reason']}\n")
                        f.write(f"- ëŒ€ìƒ: {suggestion['target']}\n")
                        f.write(f"- êµ¬í˜„: {suggestion['implementation']}\n")
            
            # ê²°ë¡ 
            f.write("\n## ê²°ë¡ \n\n")
            
            final_results = self.iteration_history[-1]["results"]
            initial_results = self.iteration_history[0]["results"]
            
            f.write(f"### ì„±ëŠ¥ ê°œì„ \n")
            f.write(f"- ì´ˆê¸° ì„±ëŠ¥: {initial_results['success_rate']:.1f}% "
                   f"(í‰ê·  {initial_results['average_correct_horses']:.2f}ë§ˆë¦¬)\n")
            f.write(f"- ìµœì¢… ì„±ëŠ¥: {final_results['success_rate']:.1f}% "
                   f"(í‰ê·  {final_results['average_correct_horses']:.2f}ë§ˆë¦¬)\n")
            f.write(f"- ì „ì²´ ê°œì„ : {final_results['success_rate'] - initial_results['success_rate']:+.1f}%p\n\n")
            
            # ìµœì  ë²„ì „
            best_iteration = max(self.iteration_history, 
                               key=lambda x: (x["results"]["success_rate"], 
                                            x["results"]["average_correct_horses"]))
            f.write(f"### ìµœê³  ì„±ëŠ¥ ë²„ì „\n")
            f.write(f"- ë²„ì „: {best_iteration['version']}\n")
            f.write(f"- ì ì¤‘ë¥ : {best_iteration['results']['success_rate']:.1f}%\n")
            f.write(f"- í‰ê·  ì ì¤‘: {best_iteration['results']['average_correct_horses']:.2f}ë§ˆë¦¬\n")
            f.write(f"- í”„ë¡¬í”„íŠ¸ ê²½ë¡œ: {best_iteration['prompt_path']}\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("\n### ê¶Œì¥ì‚¬í•­\n")
            if final_results['success_rate'] >= 40:
                f.write("- âœ… ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥\n")
            elif final_results['success_rate'] >= 30:
                f.write("- âš ï¸ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì´ë‚˜ ì¶”ê°€ ê°œì„  ì—¬ì§€ ìˆìŒ\n")
            else:
                f.write("- âŒ ëª©í‘œ ë¯¸ë‹¬ì„±, ì „ëµì  ì¬ê²€í†  í•„ìš”\n")
        
        print(f"\n\nğŸ“„ ìµœì¢… ë³´ê³ ì„œ ìƒì„±: {report_path}")
        
        # ìµœì  í”„ë¡¬í”„íŠ¸ë¥¼ ê³µì‹ ìœ„ì¹˜ì— ë³µì‚¬
        best_prompt = Path(best_iteration['prompt_path'])
        if best_prompt.exists():
            final_prompt = Path("prompts") / f"prediction-template-optimized-v2.md"
            subprocess.run(['cp', str(best_prompt), str(final_prompt)])
            print(f"ğŸ† ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì €ì¥: {final_prompt}")


def main():
    if len(sys.argv) < 2:
        print("Usage: python recursive_prompt_improvement_v2.py <base_prompt_file> [test_limit] [max_iterations] [max_workers]")
        print("Example: python recursive_prompt_improvement_v2.py prompts/prediction-template-v10.3.md 30 5 3")
        sys.exit(1)
    
    base_prompt = sys.argv[1]
    test_limit = int(sys.argv[2]) if len(sys.argv) > 2 else 30
    max_iterations = int(sys.argv[3]) if len(sys.argv) > 3 else 5
    max_workers = int(sys.argv[4]) if len(sys.argv) > 4 else 3
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(base_prompt).exists():
        print(f"Error: í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_prompt}")
        sys.exit(1)
    
    # ì¬ê·€ ê°œì„  ì‹¤í–‰
    improver = RecursivePromptImproverV2(base_prompt, max_iterations)
    improver.run_improvement_cycle(test_limit, max_workers)


if __name__ == "__main__":
    main()