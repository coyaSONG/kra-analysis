#!/usr/bin/env python3
"""
ì¬ê·€ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v5

v4ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ì—¬ ì‹¤ì œë¡œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ê°œì„ í•˜ëŠ”
ì§„ì •í•œ ì¬ê·€ ê°œì„  ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. í”„ë¡¬í”„íŠ¸ íŒŒì‹± ë° êµ¬ì¡°í™”
2. ì‹¬ì¸µ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
3. ë™ì  í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
4. ì²´ê³„ì ì¸ ì˜ˆì‹œ ê´€ë¦¬
"""

import argparse
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# v5 ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent))
from evaluation.calibration import ConfidenceCalibrator
from evaluation.data_splitter import TemporalDataSplitter
from v5_modules import (
    DynamicReconstructor,
    ExamplesManager,
    InsightAnalyzer,
    PromptParser,
)
from v5_modules.utils import (
    calculate_success_metrics,
    ensure_directory,
    format_duration,
    get_data_dir,
    get_prompts_dir,
    increment_version,
    read_json_file,
    read_text_file,
    setup_logger,
    write_text_file,
)


def should_promote_challenger(
    champion_metrics: dict[str, Any] | None,
    challenger_metrics: dict[str, Any] | None,
    leakage_passed: bool,
    selection_gate: str = "strict",
) -> dict[str, Any]:
    """ì±”í”¼ì–¸-ì±Œë¦°ì € ìŠ¹ê²© ê²Œì´íŠ¸ íŒë‹¨."""

    if champion_metrics is None:
        return {"promote": True, "reason": "initial_champion", "checks": {}}

    # ì±”í”¼ì–¸ì´ 0 ìƒ˜í”Œì´ë©´ ì‹¤ì§ˆì  ì±”í”¼ì–¸ ì—†ìŒìœ¼ë¡œ ì·¨ê¸‰
    # (log_loss=0.0 ë“± ë¬´ì˜ë¯¸í•œ ê°’ì´ "ì™„ë²½"ìœ¼ë¡œ í•´ì„ë˜ëŠ” ê²ƒì„ ë°©ì§€)
    champion_samples = int(champion_metrics.get("samples", 0))
    if champion_samples == 0:
        return {"promote": True, "reason": "champion_no_samples", "checks": {}}

    # coverage=0 (ëª¨ë“  ì˜ˆì¸¡ì´ deferred)ì¸ ê²½ìš° ë¬´ì˜ë¯¸í•œ ì§€í‘œ ë°©ì§€
    champion_coverage = float(champion_metrics.get("coverage", 1.0))
    challenger_coverage = float(challenger_metrics.get("coverage", 1.0) if challenger_metrics else 1.0)
    if champion_coverage == 0.0 and challenger_coverage == 0.0:
        # ë‘˜ ë‹¤ coverage 0ì´ë©´ success_rate ê¸°ì¤€ìœ¼ë¡œë§Œ ë¹„êµí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìŠ¹ê²© ë¶ˆê°€
        return {"promote": False, "reason": "both_zero_coverage", "checks": {}}
    if champion_coverage == 0.0:
        # ì±”í”¼ì–¸ì˜ coverageê°€ 0ì´ë©´ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ ì±Œë¦°ì € ìŠ¹ê²©
        return {"promote": True, "reason": "champion_zero_coverage", "checks": {}}
    if challenger_coverage == 0.0:
        # ì±Œë¦°ì €ì˜ coverageê°€ 0ì´ë©´ ì§€í‘œ ë¹„êµ ë¶ˆê°€ â†’ ìŠ¹ê²© ê±°ë¶€
        return {"promote": False, "reason": "challenger_zero_coverage", "checks": {}}

    if not leakage_passed:
        return {
            "promote": False,
            "reason": "leakage_check_failed",
            "checks": {"leakage_passed": False},
        }

    champion_metrics = champion_metrics or {}
    challenger_metrics = challenger_metrics or {}

    champion_log_loss = float(champion_metrics.get("log_loss", float("inf")))
    challenger_log_loss = float(challenger_metrics.get("log_loss", float("inf")))
    champion_ece = float(champion_metrics.get("ece", float("inf")))
    challenger_ece = float(challenger_metrics.get("ece", float("inf")))

    champion_top3 = float(champion_metrics.get("topk", {}).get("top_3", 0.0))
    challenger_top3 = float(challenger_metrics.get("topk", {}).get("top_3", 0.0))
    champion_roi = float(champion_metrics.get("roi", {}).get("avg_roi", 0.0))
    challenger_roi = float(challenger_metrics.get("roi", {}).get("avg_roi", 0.0))

    checks = {
        "log_loss_improved": challenger_log_loss < champion_log_loss,
        "ece_not_worse": challenger_ece <= champion_ece,
        "top3_improved": challenger_top3 > champion_top3,
        "roi_improved": challenger_roi > champion_roi,
    }

    if selection_gate == "balanced":
        required_count = 2
        score = sum(
            [
                int(checks["log_loss_improved"]),
                int(checks["ece_not_worse"]),
                int(checks["top3_improved"] or checks["roi_improved"]),
            ]
        )
        if score >= required_count:
            return {"promote": True, "reason": "balanced_gate_passed", "checks": checks}
        return {"promote": False, "reason": "balanced_gate_failed", "checks": checks}

    if not checks["log_loss_improved"]:
        return {"promote": False, "reason": "log_loss_not_improved", "checks": checks}
    if not checks["ece_not_worse"]:
        return {"promote": False, "reason": "ece_regressed", "checks": checks}
    if not (checks["top3_improved"] or checks["roi_improved"]):
        return {
            "promote": False,
            "reason": "no_top3_or_roi_improvement",
            "checks": checks,
        }

    return {"promote": True, "reason": "gate_passed", "checks": checks}


class RecursivePromptImprovementV5:
    """ì¬ê·€ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v5"""

    def __init__(
        self,
        initial_prompt_path: Path,
        target_date: str = "all",
        max_iterations: int = 5,
        parallel_count: int = 5,
        race_limit: str = None,
        metrics_profile: str = "rpi_v1",
        selection_gate: str = "strict",
        time_split: str = "rolling",
        defer_policy: str = "threshold",
        asof_check: str = "on",
        patience: int = 3,
        min_improvement: float = 0.005,
    ):
        self.initial_prompt_path = initial_prompt_path
        self.target_date = target_date
        self.max_iterations = max_iterations
        self.parallel_count = parallel_count
        self.race_limit = race_limit
        self.metrics_profile = metrics_profile
        self.selection_gate = selection_gate
        self.time_split = time_split
        self.defer_policy = defer_policy
        self.asof_check = asof_check

        # Patience-based early stopping (Phase 4)
        self.patience = patience
        self.patience_counter = 0
        self.min_improvement = min_improvement  # 0.5%p minimum improvement

        # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
        self.working_dir = (
            get_data_dir()
            / f"recursive_improvement_v5/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        ensure_directory(self.working_dir)

        # ë¡œê±° ì„¤ì •
        self.logger = setup_logger("v5_main")

        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.prompt_parser = PromptParser()
        self.insight_analyzer = InsightAnalyzer()
        self.reconstructor = DynamicReconstructor()
        self.examples_manager = ExamplesManager()
        self.data_splitter = TemporalDataSplitter()
        self.calibrator = ConfidenceCalibrator()

        # ìƒíƒœ ê´€ë¦¬
        self.iteration_history = []
        self.best_performance = 0.0
        self.best_prompt_path = None
        self.champion_metrics: dict[str, Any] | None = None
        self.champion_prompt_path: Path | None = None
        self.champion_structure = None
        self.champion_history_file = (
            get_data_dir() / "prompt_evaluation" / "champion_history.jsonl"
        )
        ensure_directory(self.champion_history_file.parent)

    def _append_champion_history(self, payload: dict[str, Any]) -> None:
        """ìŠ¹ê²©/ë¡¤ë°± ì´ë ¥ì„ jsonlë¡œ ì €ì¥."""
        with open(self.champion_history_file, "a", encoding="utf-8") as fp:
            fp.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def run(self) -> dict[str, Any]:
        """ì¬ê·€ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.info("=" * 80)
        self.logger.info("ì¬ê·€ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v5 ì‹œì‘")
        self.logger.info(f"ì´ˆê¸° í”„ë¡¬í”„íŠ¸: {self.initial_prompt_path}")
        self.logger.info(f"ëŒ€ìƒ ë‚ ì§œ: {self.target_date}")
        self.logger.info(f"ìµœëŒ€ ë°˜ë³µ: {self.max_iterations}íšŒ")
        self.logger.info("=" * 80)

        # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì½ê¸° ë° íŒŒì‹±
        current_prompt_path = self.initial_prompt_path
        current_prompt_content = read_text_file(current_prompt_path)
        current_structure = self.prompt_parser.parse(current_prompt_content)

        # ì´ˆê¸° ë²„ì „ ì„¤ì •
        if not current_structure.version:
            current_structure.version = "v1.0"

        # Train/Val/Test ì‹œê°„ìˆœ ë¶„í•  (Phase 4)
        self.data_splits = None
        try:
            from evaluation.evaluate_prompt_v3 import PromptEvaluatorV3

            temp_evaluator = PromptEvaluatorV3(
                prompt_version="split_check",
                prompt_path=str(self.initial_prompt_path),
            )
            all_races = temp_evaluator.find_test_races()
            if all_races:
                self.data_splits = self.data_splitter.split(all_races)
                self.logger.info("ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
                self.logger.info(f"  - Train: {len(self.data_splits['train'])}ê°œ")
                self.logger.info(f"  - Val: {len(self.data_splits['val'])}ê°œ")
                self.logger.info(f"  - Test: {len(self.data_splits['test'])}ê°œ")
        except Exception as e:
            self.logger.warning(f"ë°ì´í„° ë¶„í•  ì‹¤íŒ¨ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€): {e}")

        start_time = time.time()

        for iteration in range(1, self.max_iterations + 1):
            self.logger.info(f"\n{'=' * 60}")
            self.logger.info(f"ë°˜ë³µ {iteration}/{self.max_iterations} ì‹œì‘")
            self.logger.info(f"í˜„ì¬ í”„ë¡¬í”„íŠ¸: {current_structure.version}")

            iteration_start = time.time()

            # 1. í”„ë¡¬í”„íŠ¸ í‰ê°€
            self.logger.info("\n[1ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ í‰ê°€ ì¤‘...")
            evaluation_results = self._evaluate_prompt(
                current_prompt_path, version=current_structure.version
            )

            if not evaluation_results:
                self.logger.error("í‰ê°€ ì‹¤íŒ¨ - ì¤‘ë‹¨")
                break

            # ì„±ëŠ¥ ê³„ì‚°
            detailed_results = evaluation_results.get("detailed_results", [])
            metrics = calculate_success_metrics(detailed_results)
            current_performance = metrics["success_rate"]
            challenger_metrics = evaluation_results.get("metrics_v2", {})
            leakage_check = evaluation_results.get(
                "leakage_check", {"passed": True, "issues": []}
            )
            promotion_decision = should_promote_challenger(
                champion_metrics=self.champion_metrics,
                challenger_metrics=challenger_metrics,
                leakage_passed=bool(leakage_check.get("passed", True)),
                selection_gate=self.selection_gate,
            )

            self.logger.info("í‰ê°€ ì™„ë£Œ:")
            self.logger.info(f"  - ì„±ê³µë¥ : {current_performance:.1f}%")
            self.logger.info(f"  - í‰ê·  ì ì¤‘: {metrics['avg_correct']:.2f}ë§ˆë¦¬")
            self.logger.info(f"  - í‰ê°€ ê²½ì£¼ ìˆ˜: {metrics['total_races']}ê°œ")
            if challenger_metrics:
                self.logger.info(
                    "  - í’ˆì§ˆ ì§€í‘œ: "
                    f"log_loss={challenger_metrics.get('log_loss', 0):.4f}, "
                    f"ece={challenger_metrics.get('ece', 0):.4f}, "
                    f"top3={challenger_metrics.get('topk', {}).get('top_3', 0):.4f}, "
                    f"roi={challenger_metrics.get('roi', {}).get('avg_roi', 0):.4f}"
                )
            self.logger.info(
                "  - ìŠ¹ê²© íŒë‹¨: "
                f"{'ìŠ¹ê²©' if promotion_decision['promote'] else 'ìœ ì§€'} "
                f"({promotion_decision['reason']})"
            )

            # ì´ë ¥ ì €ì¥
            iteration_data = {
                "iteration": iteration,
                "version": current_structure.version,
                "performance": current_performance,
                "metrics": metrics,
                "metrics_v2": challenger_metrics,
                "leakage_check": leakage_check,
                "promotion_decision": promotion_decision,
                "prompt_path": str(current_prompt_path),
            }
            self.iteration_history.append(iteration_data)
            self._append_champion_history(
                {
                    "timestamp": datetime.now().isoformat(),
                    "iteration": iteration,
                    "version": current_structure.version,
                    "prompt_path": str(current_prompt_path),
                    "promotion_decision": promotion_decision,
                    "metrics_v2": challenger_metrics,
                    "leakage_check": leakage_check,
                }
            )

            # ì±”í”¼ì–¸ ìƒíƒœ ì—…ë°ì´íŠ¸
            if promotion_decision["promote"]:
                self.champion_metrics = challenger_metrics
                self.champion_prompt_path = current_prompt_path
                self.champion_structure = current_structure

            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (ìŠ¹ê²©ëœ í›„ë³´ ê¸°ì¤€)
            if (
                promotion_decision["promote"]
                and current_performance
                > self.best_performance + self.min_improvement * 100
            ):
                self.best_performance = current_performance
                self.best_prompt_path = current_prompt_path
                self.patience_counter = 0
                self.logger.info(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {current_performance:.1f}%")
            elif (
                promotion_decision["promote"]
                and current_performance > self.best_performance
            ):
                self.best_performance = current_performance
                self.best_prompt_path = current_prompt_path
                self.patience_counter += 1
                self.logger.info(
                    f"ğŸ“ˆ ë¯¸ë¯¸í•œ ê°œì„  ({current_performance:.1f}%), "
                    f"patience: {self.patience_counter}/{self.patience}"
                )
            else:
                self.patience_counter += 1
                self.logger.info(
                    f"ğŸ“Š ë¯¸ê°œì„ , patience: {self.patience_counter}/{self.patience}"
                )

            # ëª©í‘œ ë‹¬ì„± í™•ì¸
            if current_performance >= 70.0:
                self.logger.info("ğŸ‰ ëª©í‘œ ì„±ëŠ¥(70%) ë‹¬ì„±!")
                break

            # Patience ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ
            if self.patience_counter >= self.patience:
                self.logger.info(
                    f"â¹ ì¡°ê¸° ì¢…ë£Œ: {self.patience}íšŒ ì—°ì† ìœ ì˜ë¯¸í•œ ê°œì„  ì—†ìŒ "
                    f"(ìµœì†Œ ê°œì„  ê¸°ì¤€: {self.min_improvement * 100:.1f}%p)"
                )
                break

            # ë§ˆì§€ë§‰ ë°˜ë³µì´ë©´ ê°œì„  ì—†ì´ ì¢…ë£Œ
            if iteration == self.max_iterations:
                self.logger.info("ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬")
                break

            # 2. ì¸ì‚¬ì´íŠ¸ ë¶„ì„
            self.logger.info("\n[2ë‹¨ê³„] ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘...")

            # ì˜ˆì‹œ ì¶”ê°€ (ìµœëŒ€ 20ê°œ)
            added_count = self.examples_manager.add_examples_from_evaluation(
                detailed_results, limit=20
            )
            self.logger.info(f"  - ì˜ˆì‹œ í’€ì— {added_count}ê°œ ì¶”ê°€")

            # ì¸ì‚¬ì´íŠ¸ ë¶„ì„
            insight_analysis = self.insight_analyzer.analyze(detailed_results)

            # ë¶„ì„ ë³´ê³ ì„œ ì €ì¥
            analysis_report = self.insight_analyzer.generate_report(insight_analysis)
            analysis_path = self.working_dir / f"analysis_iteration_{iteration}.md"
            write_text_file(analysis_report, analysis_path)

            self.logger.info(
                f"  - ì£¼ìš” ë°œê²¬ì‚¬í•­: {len(insight_analysis.summary.get('key_findings', []))}ê°œ"
            )
            self.logger.info(f"  - ê¶Œê³ ì‚¬í•­: {len(insight_analysis.recommendations)}ê°œ")

            # 3. í”„ë¡¬í”„íŠ¸ ê°œì„ 
            self.logger.info("\n[3ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘...")

            if not promotion_decision["promote"] and self.champion_structure:
                self.logger.info(
                    "  - ì´ì „ ì±”í”¼ì–¸ ê¸°ì¤€ìœ¼ë¡œ ë¡¤ë°± í›„ ë‹¤ìŒ í›„ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
                )
                base_structure = self.champion_structure
            else:
                base_structure = current_structure

            # ìƒˆ ë²„ì „ ë²ˆí˜¸ ìƒì„±
            new_version = increment_version(base_structure.version)

            # í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
            new_structure, changes = self.reconstructor.reconstruct_prompt(
                base_structure, insight_analysis, new_version, metrics
            )

            # ê³ ê¸‰ ê¸°ë²• ì ìš© ìƒíƒœ ë¡œê¹…
            advanced_status = self.reconstructor.get_advanced_techniques_status(
                current_performance
            )
            applied_techniques = [
                tech for tech, applied in advanced_status.items() if applied
            ]
            if applied_techniques:
                self.logger.info(
                    f"  - ì ìš©ëœ ê³ ê¸‰ ê¸°ë²•: {', '.join(applied_techniques)}"
                )

            # ì˜ˆì‹œ ì—…ë°ì´íŠ¸
            used_example_ids = self.examples_manager.update_examples_section(
                new_structure, strategy="balanced"
            )

            # ì„±ëŠ¥ ì¶”ì 
            self.examples_manager.track_usage_performance(
                used_example_ids, current_performance
            )

            self.logger.info(f"  - ì ìš©ëœ ë³€ê²½ì‚¬í•­: {len(changes)}ê°œ")
            for change in changes[:5]:  # ìƒìœ„ 5ê°œ í‘œì‹œ
                self.logger.info(f"    â€¢ {change.description}")

            # íŠ¹ë³„íˆ ê³ ê¸‰ ê¸°ë²• ê´€ë ¨ ë³€ê²½ì‚¬í•­ ê°•ì¡°
            advanced_changes = [
                c
                for c in changes
                if any(
                    tech in c.description.lower()
                    for tech in ["thinking", "ê²€ì¦", "í† í°", "ìµœì í™”"]
                )
            ]
            if advanced_changes:
                self.logger.info("  - ê³ ê¸‰ ê¸°ë²• ë³€ê²½ì‚¬í•­:")
                for change in advanced_changes[:3]:
                    self.logger.info(f"    â˜… {change.description}")

            # ê²€ì¦
            validation_issues = self.reconstructor.validate_changes(new_structure)
            if validation_issues:
                self.logger.warning(f"ê²€ì¦ ë¬¸ì œ ë°œê²¬: {validation_issues}")

            # 4. ìƒˆ í”„ë¡¬í”„íŠ¸ ì €ì¥
            new_prompt_content = new_structure.to_prompt()
            new_prompt_filename = f"prompt_{new_version}.md"
            new_prompt_path = self.working_dir / new_prompt_filename
            write_text_file(new_prompt_content, new_prompt_path)

            # prompts í´ë”ì—ë„ ë³µì‚¬
            official_path = get_prompts_dir() / f"prediction-template-{new_version}.md"
            write_text_file(new_prompt_content, official_path)

            self.logger.info("\nê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì €ì¥:")
            self.logger.info(f"  - ì‘ì—… ê²½ë¡œ: {new_prompt_path}")
            self.logger.info(f"  - ê³µì‹ ê²½ë¡œ: {official_path}")

            # ë‹¤ìŒ ë°˜ë³µ ì¤€ë¹„
            current_prompt_path = new_prompt_path
            current_structure = new_structure

            # ë°˜ë³µ ì‹œê°„ ë¡œê·¸
            iteration_time = time.time() - iteration_start
            self.logger.info(
                f"\në°˜ë³µ {iteration} ì™„ë£Œ (ì†Œìš”ì‹œê°„: {format_duration(iteration_time)})"
            )

            # ì €ì„±ê³¼ ì˜ˆì‹œ ì •ë¦¬
            if iteration % 3 == 0:  # 3íšŒë§ˆë‹¤
                removed = self.examples_manager.cleanup_low_performers()
                if removed > 0:
                    self.logger.info(f"ì €ì„±ê³¼ ì˜ˆì‹œ {removed}ê°œ ì œê±°")

        # 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬
        total_time = time.time() - start_time

        self.logger.info("\n" + "=" * 80)
        self.logger.info("ì¬ê·€ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
        self.logger.info(f"ì´ ì†Œìš”ì‹œê°„: {format_duration(total_time)}")
        self.logger.info(
            f"ìµœê³  ì„±ëŠ¥: {self.best_performance:.1f}% ({self.best_prompt_path})"
        )

        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        final_report = self._generate_final_report()
        report_path = self.working_dir / "final_report.md"
        write_text_file(final_report, report_path)

        # ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "best_performance": self.best_performance,
            "best_prompt_path": str(self.best_prompt_path),
            "iterations": len(self.iteration_history),
            "working_dir": str(self.working_dir),
            "report_path": str(report_path),
        }

    def _evaluate_prompt(
        self, prompt_path: Path, version: str | None = None
    ) -> dict[str, Any] | None:
        """í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰"""
        try:
            # ë²„ì „ ì¶”ì¶œ (ëª…ì‹œì  ì „ë‹¬ ìš°ì„ , ì—†ìœ¼ë©´ íŒŒì‹±)
            if not version:
                prompt_content = read_text_file(prompt_path)
                structure = self.prompt_parser.parse(prompt_content)
                version = structure.version or "unknown"

            # evaluate_prompt_v3.py ì‹¤í–‰
            # ì‚¬ìš©ìê°€ ì§€ì •í•œ ê²½ì£¼ ìˆ˜ ë˜ëŠ” ìë™ ì„¤ì •
            if self.race_limit:
                if self.race_limit.lower() == "all":
                    # ì „ì²´ ê²½ì£¼ ì‚¬ìš© (ì‚¬ì‹¤ìƒ ë¬´ì œí•œ)
                    race_count = "999999"
                else:
                    # ìˆ«ìë¡œ ì§€ì •ëœ ê²½ìš°
                    race_count = self.race_limit
            else:
                # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ê²½ì£¼ ìˆ˜ë¥¼ ì¤„ì„
                # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ê²½ì£¼ë¥¼ í‰ê°€
                if self.max_iterations <= 2:
                    race_count = "5"  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
                elif self.max_iterations <= 5:
                    race_count = "50"  # ì¤‘ê°„ ê·œëª¨ í‰ê°€
                else:
                    race_count = "100"  # ëŒ€ê·œëª¨ í‰ê°€

            # uvë¥¼ í†µí•´ ì‹¤í–‰ (ê°€ìƒí™˜ê²½ ì˜ì¡´ì„± ì‚¬ìš©)
            # í”„ë¡¬í”„íŠ¸ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (subprocess cwdê°€ ë‹¤ë¥´ë¯€ë¡œ)
            absolute_prompt_path = str(Path(prompt_path).resolve())
            cmd = [
                "uv",
                "run",
                "python3",
                "evaluation/evaluate_prompt_v3.py",  # scripts_dir ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
                version,
                absolute_prompt_path,
                race_count,  # í‰ê°€í•  ê²½ì£¼ ìˆ˜
                str(self.parallel_count),
                "--report-format",
                "v2",
                "--metrics-profile",
                self.metrics_profile,
                "--asof-check",
                self.asof_check,
                "--topk",
                "1,3",
            ]
            if self.defer_policy == "threshold":
                # 0.4ë¡œ ë‚®ì¶¤: Claude ì˜ˆì¸¡ confidenceê°€ ë³´í†µ 50-65% (ì •ê·œí™” í›„ 0.5-0.65)
                # 0.7ì€ ê±°ì˜ ëª¨ë“  ì˜ˆì¸¡ì„ deferred ì²˜ë¦¬í•˜ì—¬ metrics_v2ê°€ 0ì´ ë¨
                cmd.extend(["--defer-threshold", "0.4"])

            if self.target_date != "all":
                # íŠ¹ì • ë‚ ì§œë§Œ í‰ê°€í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
                pass

            self.logger.info(f"í‰ê°€ ëª…ë ¹: {' '.join(cmd)}")
            if race_count == "999999":
                self.logger.info("í‰ê°€í•  ê²½ì£¼ ìˆ˜: ì „ì²´ (ì œí•œ ì—†ìŒ)")
            else:
                self.logger.info(f"í‰ê°€í•  ê²½ì£¼ ìˆ˜: {race_count}ê°œ")

            # ì‹¤í–‰ (packages/scripts ë””ë ‰í† ë¦¬ì—ì„œ)
            scripts_dir = Path(__file__).parent.parent  # prompt_improvement -> scripts

            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (CLAUDE_CODE=1ë¡œ zoxide ì¶©ëŒ ë°©ì§€)
            env = os.environ.copy()
            env["CLAUDE_CODE"] = "1"

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(scripts_dir),
                env=env,
            )

            if result.returncode != 0:
                self.logger.error(f"í‰ê°€ ì‹¤íŒ¨: {result.stderr}")
                return None

            # subprocess ì¶œë ¥ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
            if result.stderr:
                for line in result.stderr.strip().split("\n")[-5:]:
                    self.logger.debug(f"  í‰ê°€ stderr: {line}")

            # ì ì‹œ ëŒ€ê¸° (íŒŒì¼ ìƒì„± ì‹œê°„)
            time.sleep(2)

            # ê²°ê³¼ íŒŒì¼ ì°¾ê¸° - ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒì„±ëœ íŒŒì¼ë§Œ ì‚¬ìš©
            eval_dir = get_data_dir() / "prompt_evaluation"
            eval_files = list(eval_dir.glob(f"evaluation_{version}_*.json"))

            self.logger.info(f"í‰ê°€ ë””ë ‰í† ë¦¬: {eval_dir}")
            self.logger.info(f"ê²€ìƒ‰ íŒ¨í„´: evaluation_{version}_*.json")
            self.logger.info(f"ì°¾ì€ íŒŒì¼ ìˆ˜: {len(eval_files)}")

            if not eval_files:
                self.logger.error(
                    f"í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: evaluation_{version}_*.json"
                )
                # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
                all_files = list(eval_dir.glob("evaluation_*.json"))
                self.logger.info(f"ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í‰ê°€ íŒŒì¼: {len(all_files)}ê°œ")
                if all_files:
                    self.logger.info(f"ìµœê·¼ íŒŒì¼ ì˜ˆì‹œ: {all_files[-1].name}")
                return None

            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì½ê¸°
            latest_file = max(eval_files, key=lambda p: p.stat().st_mtime)
            eval_data = read_json_file(latest_file)

            # 0 ìƒ˜í”Œ ê²°ê³¼ ê²€ì¦ - ìœ íš¨ ì˜ˆì¸¡ì´ ì—†ìœ¼ë©´ ì¬ì‹œë„ ë˜ëŠ” ì‹¤íŒ¨ ì²˜ë¦¬
            valid_predictions = eval_data.get("valid_predictions", 0)
            if valid_predictions == 0:
                self.logger.warning(
                    f"âš ï¸ í‰ê°€ ê²°ê³¼ì— ìœ íš¨ ì˜ˆì¸¡ì´ 0ê±´ì…ë‹ˆë‹¤ (íŒŒì¼: {latest_file.name}). "
                    "í”„ë¡¬í”„íŠ¸ ê²½ë¡œ ë˜ëŠ” Claude CLI ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”."
                )

            return eval_data

        except Exception as e:
            self.logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _generate_final_report(self) -> str:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report = []

        report.append("# ì¬ê·€ í”„ë¡¬í”„íŠ¸ ê°œì„  v5 ìµœì¢… ë³´ê³ ì„œ\n")
        report.append(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # ìš”ì•½
        report.append("## ìš”ì•½")
        report.append(f"- ì´ˆê¸° í”„ë¡¬í”„íŠ¸: {self.initial_prompt_path}")
        report.append(f"- ì´ ë°˜ë³µ íšŸìˆ˜: {len(self.iteration_history)}")
        report.append(f"- ìµœê³  ì„±ëŠ¥: {self.best_performance:.1f}%")
        report.append(f"- ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸: {self.best_prompt_path}")

        # ì„±ëŠ¥ ì¶”ì´
        report.append("\n## ì„±ëŠ¥ ì¶”ì´")
        report.append("| ë°˜ë³µ | ë²„ì „ | ì„±ê³µë¥  | í‰ê·  ì ì¤‘ | ìŠ¹ê²© |")
        report.append("|------|------|--------|-----------|------|")

        for item in self.iteration_history:
            promoted = "Y" if item.get("promotion_decision", {}).get("promote") else "N"
            report.append(
                f"| {item['iteration']} | {item['version']} | "
                f"{item['performance']:.1f}% | "
                f"{item['metrics']['avg_correct']:.2f}ë§ˆë¦¬ | "
                f"{promoted} |"
            )

        # ê°œì„  ë‚´ì—­
        report.append("\n## ì£¼ìš” ê°œì„  ë‚´ì—­")

        # ë³€ê²½ ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
        change_report = self.reconstructor.generate_change_report()
        report.append(change_report)

        # ì˜ˆì‹œ í†µê³„
        report.append("\n## ì˜ˆì‹œ ê´€ë¦¬ í†µê³„")
        stats = self.examples_manager.get_statistics()
        report.append(f"- ì´ ì˜ˆì‹œ ìˆ˜: {stats['total_examples']}")
        report.append(f"- ì„±ê³µ ì˜ˆì‹œ: {stats['success_examples']}")
        report.append(f"- ì‹¤íŒ¨ ì˜ˆì‹œ: {stats['failure_examples']}")
        report.append(f"- í‰ê·  ì„±ê³¼: {stats['avg_performance']:.1f}%")

        # ê²°ë¡ 
        report.append("\n## ê²°ë¡ ")

        if self.best_performance >= 70.0:
            report.append("âœ… **ëª©í‘œ ë‹¬ì„±**: 70% ì´ìƒì˜ ì„±ê³µë¥ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
        else:
            improvement = (
                self.best_performance - self.iteration_history[0]["performance"]
            )
            report.append(f"ğŸ“ˆ **ê°œì„  ì„±ê³¼**: {improvement:+.1f}% í–¥ìƒ")
            report.append(
                f"   (ì´ˆê¸°: {self.iteration_history[0]['performance']:.1f}% â†’ ìµœì¢…: {self.best_performance:.1f}%)"
            )

        # v4ì™€ì˜ ì°¨ì´ì 
        report.append("\n## v4 ëŒ€ë¹„ ê°œì„ ì‚¬í•­")
        report.append("1. **ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ê°œì„ **: ë‹¨ìˆœ ì˜ˆì‹œ ë³€ê²½ì´ ì•„ë‹Œ êµ¬ì¡°ì  ê°œì„ ")
        report.append("2. **ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜**: ë°ì´í„° ë¶„ì„ì— ê¸°ë°˜í•œ êµ¬ì²´ì  ê°œì„ ")
        report.append("3. **ì²´ê³„ì  ì˜ˆì‹œ ê´€ë¦¬**: ì„±ê³¼ ì¶”ì  ë° ìµœì  ì„ íƒ")
        report.append("4. **íˆ¬ëª…í•œ ë³€ê²½ ì¶”ì **: ëª¨ë“  ë³€ê²½ì‚¬í•­ ê¸°ë¡ ë° ê²€ì¦")
        report.append(
            "5. **ê³ ê¸‰ ê¸°ë²• í†µí•©**: Extended Thinking, ê°•í™”ëœ ê²€ì¦, í† í° ìµœì í™”"
        )

        # ê³ ê¸‰ ê¸°ë²• ì‚¬ìš© ë‚´ì—­
        report.append("\n## ê³ ê¸‰ ê¸°ë²• ì ìš© ë‚´ì—­")
        report.append("### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ ê¸°ë°˜ ê°œì„ ")

        # ê° ë°˜ë³µì—ì„œ ì ìš©ëœ ê¸°ë²• ì¶”ì 
        techniques_used = set()
        for item in self.iteration_history:
            perf = item["performance"]
            status = self.reconstructor.get_advanced_techniques_status(perf)
            for tech, applied in status.items():
                if applied:
                    techniques_used.add(tech)

        if "extended_thinking" in techniques_used:
            report.append(
                "- **Extended Thinking Mode**: ì €ì„±ê³¼ êµ¬ê°„ì—ì„œ ultrathink í‚¤ì›Œë“œ ì ìš©"
            )
        if "self_verification" in techniques_used:
            report.append(
                "- **ê°•í™”ëœ ìê°€ ê²€ì¦**: ë‹¤ë‹¨ê³„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ë° ì˜¤ë¥˜ ë³µêµ¬ ê°€ì´ë“œ ì¶”ê°€"
            )
        if "token_optimization" in techniques_used:
            report.append(
                "- **í† í° ìµœì í™”**: ì¤‘ë³µ ì œê±°, í‘œ í˜•ì‹ ë„ì…, ì•½ì–´ ì‚¬ìš©ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ"
            )

        return "\n".join(report)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ì¬ê·€ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ v5",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (base-prompt-v1.0.md, ëª¨ë“  ë‚ ì§œ, 5íšŒ ë°˜ë³µ)
  python3 recursive_prompt_improvement_v5.py

  # íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¡œ ì‹œì‘
  python3 recursive_prompt_improvement_v5.py prompts/prediction-template-v2.1.md

  # 10íšŒ ë°˜ë³µ, ë³‘ë ¬ ì²˜ë¦¬ 10ê°œ
  python3 recursive_prompt_improvement_v5.py -i 10 -p 10

  # 200ê°œ ê²½ì£¼ë¡œ í‰ê°€
  python3 recursive_prompt_improvement_v5.py -r 200

  # ì „ì²´ ê²½ì£¼ ì‚¬ìš©
  python3 recursive_prompt_improvement_v5.py -r all

  # ì „ì²´ ì˜µì…˜ ì‚¬ìš©
  python3 recursive_prompt_improvement_v5.py -i 5 -p 15 -r 150
        """,
    )

    parser.add_argument(
        "prompt_path",
        nargs="?",
        default="prompts/base-prompt-v1.0.md",
        help="ì´ˆê¸° í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: prompts/base-prompt-v1.0.md)",
    )

    parser.add_argument(
        "target_date",
        nargs="?",
        default="all",
        help="í‰ê°€ ëŒ€ìƒ ë‚ ì§œ (YYYYMMDD ë˜ëŠ” all, ê¸°ë³¸ê°’: all)",
    )

    parser.add_argument(
        "-i", "--iterations", type=int, default=5, help="ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )

    parser.add_argument(
        "-p", "--parallel", type=int, default=5, help="ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )

    parser.add_argument(
        "-r",
        "--races",
        type=str,
        default=None,
        help="í‰ê°€í•  ê²½ì£¼ ìˆ˜ (ê¸°ë³¸ê°’: ìë™ - ë°˜ë³µìˆ˜ì— ë”°ë¼ 5/50/100, 'all': ì „ì²´ ê²½ì£¼)",
    )
    parser.add_argument(
        "--metrics-profile",
        choices=["rpi_v1"],
        default="rpi_v1",
        help="í‰ê°€ ì§€í‘œ í”„ë¡œíŒŒì¼ (ê¸°ë³¸ê°’: rpi_v1)",
    )
    parser.add_argument(
        "--selection-gate",
        choices=["strict", "balanced"],
        default="strict",
        help="ì±”í”¼ì–¸ ìŠ¹ê²© ê²Œì´íŠ¸ (ê¸°ë³¸ê°’: strict)",
    )
    parser.add_argument(
        "--time-split",
        choices=["rolling", "holdout"],
        default="rolling",
        help="ì‹œê³„ì—´ ë¶„í•  ì „ëµ ë©”íƒ€ ì •ë³´ (ê¸°ë³¸ê°’: rolling)",
    )
    parser.add_argument(
        "--defer-policy",
        choices=["off", "threshold", "conformal-lite"],
        default="threshold",
        help="ë””í¼ ì •ì±… ë©”íƒ€ ì •ë³´ (ê¸°ë³¸ê°’: threshold)",
    )
    parser.add_argument(
        "--asof-check",
        choices=["on", "off"],
        default="on",
        help="ëˆ„ìˆ˜(as-of) ê²€ì‚¬ on/off (ê¸°ë³¸ê°’: on)",
    )
    parser.add_argument(
        "--patience",
        type=int,
        default=3,
        help="ì¡°ê¸° ì¢…ë£Œ patience (ì—°ì† ë¯¸ê°œì„  í—ˆìš© íšŸìˆ˜, ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--min-improvement",
        type=float,
        default=0.005,
        help="ìµœì†Œ ìœ ì˜ë¯¸í•œ ê°œì„ í­ (ë¹„ìœ¨, ê¸°ë³¸ê°’: 0.005 = 0.5%%p)",
    )

    args = parser.parse_args()

    # ê²½ë¡œ ê²€ì¦
    prompt_path = Path(args.prompt_path)
    if not prompt_path.exists():
        print(f"ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
        sys.exit(1)

    # v5 ì‹œìŠ¤í…œ ì‹¤í–‰
    system = RecursivePromptImprovementV5(
        initial_prompt_path=prompt_path,
        target_date=args.target_date,
        max_iterations=args.iterations,
        parallel_count=args.parallel,
        race_limit=args.races,
        metrics_profile=args.metrics_profile,
        selection_gate=args.selection_gate,
        time_split=args.time_split,
        defer_policy=args.defer_policy,
        asof_check=args.asof_check,
        patience=args.patience,
        min_improvement=args.min_improvement,
    )

    try:
        result = system.run()

        if result["success"]:
            print("\nâœ… ì¬ê·€ ê°œì„  ì™„ë£Œ!")
            print(f"   ìµœê³  ì„±ëŠ¥: {result['best_performance']:.1f}%")
            print(f"   ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸: {result['best_prompt_path']}")
            print(f"   ë³´ê³ ì„œ: {result['report_path']}")
        else:
            print("\nâŒ ì¬ê·€ ê°œì„  ì‹¤íŒ¨")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(130)
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
