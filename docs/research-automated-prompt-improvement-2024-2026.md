# Automated Prompt Improvement Loops for LLM-Based Prediction Systems

## Research Report (2024-2026 State of the Art)

**Date**: 2026-02-15
**Context**: KRA horse racing prediction system -- optimizing prompts for trifecta (1-3 finish) prediction

---

## Table of Contents

1. [Prompt Auto-Improvement Loop Design](#1-prompt-auto-improvement-loop-design)
2. [Evaluation Metrics for Prediction Quality](#2-evaluation-metrics-for-prediction-quality)
3. [Ensemble / Multi-Prompt Consensus Strategies](#3-ensemble--multi-prompt-consensus-strategies)
4. [Comparative Summary Table](#4-comparative-summary-table)
5. [Recommended Architecture for KRA System](#5-recommended-architecture-for-kra-system)
6. [Sources](#6-sources)

---

## 1. Prompt Auto-Improvement Loop Design

### 1.1 DSPy Framework (MIPROv2, BootstrapFewShot, COPRO)

**Description**: DSPy (Stanford) treats prompts as programs with typed signatures. Instead of writing prompts by hand, you define modules with input/output contracts and let optimizers ("teleprompters") find the best instructions and few-shot examples automatically. The framework separates the program logic from the prompt text, enabling systematic optimization.

**Key Optimizers**:
- **MIPROv2** (Multiprompt Instruction PRoposal Optimizer v2): The flagship optimizer. Uses a three-stage process: (1) bootstrap traces by running the program across inputs and filtering high-scoring trajectories, (2) grounded proposal stage where the LLM drafts candidate instructions using program code, data, and traces, (3) Bayesian Optimization to search the combinatorial space of instruction + example combinations. Even with modest constraints (10 instruction variants, 5 examples from pool of 50), there are ~20 million configurations -- Bayesian optimization makes this tractable.
- **BootstrapFewShot**: Automatically selects the best few-shot examples from training data by running the program and keeping trajectories that score well.
- **COPRO**: Generates and refines instructions using coordinate ascent (hill-climbing) with a metric function.

**Quantitative Results**:
- Prompts optimized with MIPROv2 perform up to 13% better than hand-crafted alternatives (Stanford 2024 evaluation across 5 multi-stage programs)
- DSPy-boosted agent tasks improved by up to 20 percentage points; some tasks (contradiction detection) went from 46% to 64% accuracy
- The DSPy roadmap targets optimizers that extract 20% more than MIPROv2 under constrained conditions

**Implementation Complexity**: Medium
- Requires restructuring prompts as DSPy modules with signatures
- Needs labeled training/validation data (ideally 100+ examples)
- MIPROv2 auto="light" takes 60-90 minutes even on modest tasks
- Well-documented Python library with extensive community support

**Key Risks/Limitations**:
- Optimization can be slow and expensive in API costs (many LLM calls during search)
- Requires a clear, computable metric function -- not always trivial for ranking tasks
- May overfit to training distribution if validation set is small
- The "programming, not prompting" paradigm requires significant refactoring of existing systems

**Source**: https://dspy.ai | arXiv:2406.11695 (Opsahl-Ong et al., 2024)

---

### 1.2 TextGrad (Automatic "Differentiation" via Text)

**Description**: TextGrad (Stanford, published in Nature 2025) applies the principle of automatic differentiation to text. It treats the entire AI system as a computation graph where nodes are text variables and edges are LLM calls. Instead of numeric gradients, it uses natural language feedback ("textual gradients") generated by an LLM to identify which components contributed to suboptimal output. These critiques propagate backward through the graph, guiding refinements.

**How It Works**:
1. Forward pass: run the prompt through the LLM pipeline
2. Loss computation: evaluate output quality (via LLM judge or metric)
3. Backward pass: LLM generates textual feedback identifying what to improve
4. Update: apply textual gradients to modify the prompt

**Quantitative Results**:
- Improved ChatGPT (GPT-3.5) from 78% to 92% accuracy on language benchmarks with just a few iterations
- On LeetCode Hard: 0-shot 26% completion rate improved to 36% with TextGrad (vs 31% for Reflexion)
- Outperformed DSPy by 7% on Object Counting task
- TextGrad and DSPy make complementary adjustments (DSPy adds examples; TextGrad optimizes instructions)

**Implementation Complexity**: Medium
- PyTorch-like API (`pip install textgrad`)
- Supports OpenAI, Anthropic, and vLLM backends
- Minibatch stochastic gradient descent for prompt optimization
- Clear tutorials available

**Key Risks/Limitations**:
- Requires a separate "backward engine" LLM to generate gradients -- adds cost
- Better suited for complex reasoning tasks than simple classification
- Prompt optimization requires multiple forward+backward passes per iteration
- Quality of textual gradients depends heavily on the critic LLM's capability

**Source**: arXiv:2406.07496 | GitHub: zou-group/textgrad

---

### 1.3 OPRO (Optimization by PROmpting) -- Google DeepMind

**Description**: OPRO uses LLMs themselves as optimizers. The core idea is a "meta-prompt" containing: (1) previously generated prompts paired with their training accuracy scores, and (2) a problem description with exemplars. The LLM generates new candidate prompts at each step, which are evaluated and added to the meta-prompt for the next iteration. This creates a self-improving trajectory where the LLM learns from the optimization history.

**How It Works**:
1. Start with seed prompts (even simple/bad ones work)
2. Build meta-prompt with top-20 performing prompt-score pairs
3. LLM generates new candidate prompts
4. Evaluate candidates on training set
5. Add to history and repeat

**Quantitative Results**:
- Outperformed human-designed prompts by up to 8% on GSM8K (math reasoning)
- Up to 50% improvement on Big-Bench Hard tasks
- OPRO discovered "Take a deep breath and work on this problem step-by-step" which outperformed "Let's think step by step"

**Implementation Complexity**: Low
- Conceptually simple -- just requires a meta-prompt template and evaluation loop
- No special libraries needed
- Works with any API-based LLM

**Key Risks/Limitations**:
- Highly dependent on model capability -- limited effectiveness with small LLMs (LLaMa-2, Mistral 7B showed poor results per ACL 2024 findings)
- Very high token consumption -- the scorer evaluation at each step is expensive
- Computational time far exceeds alternative methods
- Can get stuck in local optima for complex optimization landscapes
- Context window limits the number of historical prompts that can be included

**Source**: arXiv:2309.03409 | OpenReview: ICLR 2024

---

### 1.4 EvoPrompt (Evolutionary Prompt Optimization)

**Description**: EvoPrompt (ICLR 2024) bridges LLMs and evolutionary algorithms (EAs). It maintains a population of prompts and applies evolutionary operators -- selection, crossover, and mutation -- implemented via LLM calls rather than traditional genetic operators. Two variants exist: EvoPrompt(GA) using genetic algorithm operators and EvoPrompt(DE) using differential evolution.

**How It Works**:
1. Initialize population of N prompts (can include human-written seeds)
2. For each generation:
   - Select parent prompts based on fitness (roulette wheel selection)
   - Use LLM to perform crossover/mutation (e.g., "combine the best aspects of prompt A and B")
   - Evaluate offspring on development set
3. Keep top-N prompts, discard rest
4. Repeat for T generations

**Quantitative Results**:
- Outperformed human-engineered prompts significantly across 31 datasets
- Up to 25% improvement on Big-Bench Hard over APE and manual baselines
- Over 3 points improvement in SARI scores for text simplification
- Gradient-free: works with any black-box LLM API

**Implementation Complexity**: Low-Medium
- Straightforward evolutionary loop
- No gradients or model internals needed
- GitHub implementation available (beeevita/EvoPrompt)
- Population size and generation count are main hyperparameters

**Key Risks/Limitations**:
- Requires a fitness function (evaluation metric) -- manual design needed
- Population diversity can collapse over generations
- More random exploration than guided methods -- may waste API calls on poor candidates
- Prompt mutations may produce incoherent results without proper constraints

**Source**: arXiv:2309.08532 | ICLR 2024

---

### 1.5 APE (Automatic Prompt Engineer) & PE2

**Description**: APE (Zhou et al., 2022) was one of the first frameworks to formalize prompt engineering as a black-box optimization problem. It uses one LLM to generate candidate prompts from task descriptions/examples, evaluates them on a scoring function, and selects the best. PE2 (Ye et al., 2023) extends APE by adding stepwise reasoning, context fields, and self-critique capability to the meta-prompt.

**APE Process**:
1. Provide task examples to a "proposal" LLM
2. LLM generates multiple candidate instructions
3. Evaluate each candidate on held-out data
4. Select top performers
5. (Iterative APE): use top prompts to generate paraphrases, re-evaluate

**PE2 Improvements**:
- Uses meta-prompts with explicit stepwise reasoning for prompt proposal
- Inspects batch of failures to generate feedback ("textual gradients")
- Produces new prompts based on identified weaknesses
- Yields consistently higher gains than heuristic chains

**Quantitative Results**:
- APE discovered "Let's work this out in a step by step way to be sure we have the right answer" -- outperforming hand-crafted CoT prompts
- ProTeGi (a related gradient-descent-based approach) improved task performance by up to 31%

**Implementation Complexity**: Low
- Simplest of all methods -- just LLM calls + evaluation
- Can be implemented from scratch in <200 lines of code
- No special libraries required

**Key Risks/Limitations**:
- Basic APE is initialization-only (one-shot search, no iterative refinement)
- Limited exploration of prompt space compared to evolutionary or Bayesian methods
- Hallucination in prompt proposals can waste evaluation budget
- PE2's effectiveness depends heavily on meta-prompt quality

**Source**: arXiv:2211.01910 (APE) | ACL Findings 2024 (PE2)

---

### 1.6 PromptBreeder (Self-Referential Self-Improvement)

**Description**: PromptBreeder (DeepMind, 2023) evolves prompts using a dual-layer self-referential mechanism. It not only evolves task-prompts but also evolves the mutation-prompts that govern how task-prompts are modified. This recursive improvement is analogous to an organism refining its own genetic mutation processes.

**Mutation Operators**:
- Direct Mutation: LLM directly modifies task prompts
- Estimation of Distribution Mutation: generates prompts from distribution of successful ones
- Hypermutation: evolves the mutation-prompts themselves
- Lamarckian Mutation: reverse-engineers prompts from successful outputs
- Prompt Crossover: combines elements from two successful prompts
- Context Shuffling: varies the thinking style

**Quantitative Results**:
- Outperforms Chain-of-Thought and Plan-and-Solve Prompting on arithmetic and commonsense reasoning benchmarks
- Effective for complex tasks like hate speech classification where standard CoT fails
- The self-referential mechanism prevents stagnation that affects fixed-mutation approaches

**Implementation Complexity**: High
- Complex system with multiple mutation operators
- Requires careful configuration of population size, generation count, and mutation probabilities
- Higher API costs due to multi-layered evolution (evolving both task-prompts AND mutation-prompts)
- Typical configuration: 5 mutation prompts, 5 thinking styles, 20 generations

**Key Risks/Limitations**:
- High computational cost (many LLM calls per generation)
- Randomness can lead to inconsistent results across runs
- Evolved prompts may be opaque/hard to interpret
- No guarantee of convergence -- stochastic process

**Source**: arXiv:2309.16797 | NeurIPS 2023

---

### 1.7 PromptWizard (Microsoft Research)

**Description**: PromptWizard (Microsoft Research, 2024) uses a feedback-driven critique-and-synthesis mechanism to jointly optimize both prompt instructions and in-context examples. Unlike random or mutation-based strategies, PW systematically balances exploration and exploitation.

**Two-Stage Process**:
- **Stage 1: Iterative Instruction Optimization**
  - Generate prompt variants
  - Critique: identify weaknesses and gaps in current instruction
  - Synthesize: leverage feedback to refine the instruction
  - Repeat for multiple rounds
- **Stage 2: Joint Optimization of Instructions + Examples**
  - Combine refined prompt with carefully selected examples
  - Critique: analyze selected examples for effectiveness
  - Synthesize: generate new synthetic examples that are more diverse and task-relevant
  - Optimize both components simultaneously

**Quantitative Results**:
- Superior performance across 45 tasks
- Outperforms APO, PromptAgent, and DSPy in both accuracy AND number of API calls
- Reduces prompt optimization from months of manual work to minutes
- Deployed in production at Physics Wallah (education platform)

**Implementation Complexity**: Medium
- Open-source GitHub implementation (microsoft/PromptWizard)
- Configuration via YAML (mutate_refine_iterations, mutation_rounds, etc.)
- Requires training examples for evaluation
- Supports multiple LLM backends

**Key Risks/Limitations**:
- Joint optimization adds complexity vs instruction-only approaches
- Synthesized examples may not reflect real data distribution
- Performance gains plateau after certain number of iterations
- Requires careful tuning of hyperparameters (mutation_rounds, refine_task_eg_iterations)

**Source**: arXiv:2405.18369 | ACL Findings 2025

---

### 1.8 Self-Refine (Iterative Self-Refinement)

**Description**: Self-Refine (Madaan et al., 2023) uses the LLM as both critic and writer. After generating an initial output, the model generates feedback on that output, then produces an improved version. This loop repeats for multiple iterations. The approach is the simplest form of prompt improvement that does not require external tools.

**Process**:
1. Generate initial response
2. LLM critiques its own response (identifies errors, gaps, improvements)
3. LLM generates refined response incorporating feedback
4. Repeat steps 2-3 for N iterations

**Quantitative Results**:
- Improvements across 7 diverse tasks including code generation, math, and dialogue
- Typical improvement of 5-20% across iterations
- Most improvement comes in first 1-2 iterations; diminishing returns afterward

**Implementation Complexity**: Low
- Trivial to implement: just two additional LLM calls per iteration (critique + refine)
- No training data required
- Works with any LLM

**Key Risks/Limitations**:
- Self-refinement without external feedback is heavily contested in literature
- LLMs may not reliably identify their own errors (especially systematic biases)
- Can introduce new errors while fixing old ones
- Gains may be attributed to prompt design rather than genuine self-correction
- Diminishing returns after 2-3 iterations

**Source**: arXiv:2303.17651 | Various follow-up studies (2024)

---

### 1.9 metaTextGrad (Meta-Optimization, 2025)

**Description**: metaTextGrad is a meta-optimization framework that enhances existing LLM optimizers by automatically optimizing the optimizer itself. It introduces two components: (1) a meta prompt optimizer that refines optimizer prompts for better task adaptation, and (2) a meta structure optimizer that determines the optimal combination of different optimizers.

**Quantitative Results**:
- Outperforms zero-shot CoT, few-shot CoT, self-consistency, best-of-N, MIPROv2, and TextGrad baselines
- Particularly effective when combining complementary optimization strategies

**Implementation Complexity**: High
- Meta-level optimization adds another layer of complexity and cost
- Requires understanding of multiple base optimizers

**Source**: arXiv:2505.18524

---

### 1.10 GReaTer (Gradients over Reasoning, ICLR 2025)

**Description**: GReaTer directly incorporates gradient information over task-specific reasoning to optimize prompts. Unlike TextGrad (which uses textual feedback), GReaTer uses actual numerical task loss gradients to guide prompt optimization, enabling smaller/lightweight LLMs to self-optimize without needing costly closed-source LLMs.

**Key Advantage**: Works with open-source models, making it more cost-effective. Optimized prompts show better transferability across models.

**Implementation Complexity**: Medium-High (requires gradient access to model)

**Limitation**: Not applicable to pure API-based models (requires model weights)

**Source**: OpenReview, ICLR 2025

---

### 1.11 Comparison of Prompt Optimization Frameworks

| Framework | Approach | Needs Training Data | API-Only Compatible | Typical Improvement | API Cost |
|-----------|----------|-------------------|-------------------|-------------------|----------|
| DSPy MIPROv2 | Bayesian Optimization | Yes (100+) | Yes | +13-20% | High |
| TextGrad | Textual Gradients | Yes (small batch) | Yes | +14% (78->92%) | Medium-High |
| OPRO | Meta-prompt History | Yes (examples) | Yes | +8-50% | Very High |
| EvoPrompt | Evolutionary (GA/DE) | Yes (dev set) | Yes | Up to +25% | Medium |
| APE/PE2 | Generate + Select | Yes (examples) | Yes | Variable | Low-Medium |
| PromptBreeder | Self-Referential Evolution | Yes (training set) | Yes | Beats CoT/PoS | High |
| PromptWizard | Critique + Synthesis | Yes (examples) | Yes | SOTA on 45 tasks | Medium |
| Self-Refine | Self-Critique Loop | No | Yes | +5-20% | Low |
| metaTextGrad | Meta-optimization | Yes | Yes | Beats MIPROv2 | Very High |
| GReaTer | Numerical Gradients | Yes | No (needs weights) | Strong | Low |

---

## 2. Evaluation Metrics for Prediction Quality

### 2.1 Beyond Accuracy: Proper Scoring Rules

For a prediction system like KRA trifecta prediction, raw accuracy ("did we get it right?") is insufficient. The system needs calibrated probability estimates because:
- The same accuracy can come from overconfident wrong predictions + underconfident correct ones
- Betting/decision-making requires knowing HOW confident the model is
- Proper scoring rules incentivize honest probability reporting

#### Brier Score

**Formula**: BS = (1/N) * sum((p_i - o_i)^2) for i = 1..N

Where p_i is predicted probability and o_i is the binary outcome (0 or 1).

**Key Properties**:
- Strictly proper scoring rule: optimal strategy is to report true beliefs
- Range: 0 (perfect) to 1 (worst)
- Baseline: random guessing yields 0.25 for binary events
- Decomposes into: Calibration Loss + Grouping Loss + Irreducible Loss

**Recent LLM Forecasting Results** (from "Approaching Human-Level Forecasting with Language Models"):
- Human crowd aggregate: 0.149 Brier score
- Best LLM system (GPT-4 based): 0.179 Brier score
- GPT-3 level models: worse than random (>0.25) due to overconfidence
- Key finding: weighted average of LLM + human predictions beats both alone

**Relevance to KRA**: For multi-outcome prediction (N horses), use the multi-class extension. Critical for assessing whether the model's confidence in its top-3 predictions is well-calibrated.

#### Log Loss (Cross-Entropy Loss)

**Formula**: LL = -(1/N) * sum(o_i * log(p_i) + (1-o_i) * log(1-p_i))

**Key Properties**:
- More punishing of confident wrong predictions than Brier score
- Proper scoring rule
- Particularly useful for detecting overconfident models
- Used widely in ML model evaluation

**Relevance to KRA**: More sensitive to the model claiming "99% sure Horse A will win" when Horse A does not win. Good for penalizing overconfidence in predictions.

---

### 2.2 Expected Calibration Error (ECE)

**Formula**: ECE = sum(|B_m|/n) * |acc(B_m) - conf(B_m)| for m = 1..M

Where B_m is the set of predictions in the m-th confidence bin.

**Interpretation**: Measures how well predicted probabilities align with actual outcomes. A model predicting 70% should be correct ~70% of the time.

**Recent LLM Findings**:
- Strong models typically achieve ECE around 0.05-0.10
- Weaker models fall in the 0.15-0.25 range
- ConfTuner (NeurIPS 2025) introduced "tokenized Brier score" loss for training calibrated LLMs -- proven to be a proper scoring rule
- RLCR approach showed that augmenting binary correctness with Brier score reward substantially improves calibration with no accuracy loss

---

### 2.3 Top-K Accuracy and Ranking Metrics

For KRA's trifecta prediction (predicting 1st, 2nd, 3rd place), standard metrics include:

#### Top-K Exact Match
- Does the predicted set of K horses match the actual top-K finishers?
- Strict metric: requires exact match of all K positions
- Relaxed variant: any K correct horses regardless of order

#### NDCG@K (Normalized Discounted Cumulative Gain)
- Standard information retrieval metric adapted for ranking
- Rewards correct predictions at higher positions more than lower ones
- NDCG@3 specifically for trifecta prediction
- Recent work (ICLR 2025) proposed SoftmaxLoss@K as a differentiable surrogate for NDCG@K optimization

#### Hit Rate (HR@K)
- Binary: is the correct answer in the top-K predictions?
- For KRA: is each of the actual top-3 finishers in the predicted top-N?

#### Learning-to-Rank for Horse Racing
- Recent work (Korean Journal of Applied Statistics, 2024) showed pair-wise learning approaches (CatBoost Ranker, LambdaMART) outperform point-wise approaches for Seoul horse racing predictions
- Key predictive features: horse performance history, previous race records, starting training data, health/disease diagnoses

---

### 2.4 ROI-Based Evaluation for Betting Systems

**Critical Finding** (Walsh & Joshi, 2024 -- Machine Learning with Applications):
> Calibration-based model selection leads to dramatically higher betting returns than accuracy-based selection:
> - Calibration-optimized model: +34.69% ROI (average), +36.93% (best case)
> - Accuracy-optimized model: -35.17% ROI (average), +5.56% (best case)
> - Calibration-optimized model generated 69.86% higher average returns

**Kelly Criterion**: The optimal bet sizing strategy (Kelly betting) ONLY works with a well-calibrated model. With miscalibrated probabilities, Kelly betting leads to ruin.

**Practical ROI Metrics for KRA**:
1. **Flat-bet ROI**: Return assuming equal bet size on every prediction
2. **Kelly ROI**: Return using Kelly criterion for bet sizing based on model confidence
3. **Expected Value (EV)**: predicted_probability * payout - 1
4. **Yield**: total_profit / total_wagered * 100

**Key Insight**: Even a model with moderate accuracy but excellent calibration can be profitable, while a highly accurate but poorly calibrated model will lose money over time.

---

### 2.5 Calibration Methods

#### Platt Scaling (Parametric)
- Fits a logistic regression on model outputs to learn a calibration mapping
- Simple: only 2 parameters to learn
- Works well when miscalibration is systematic (e.g., consistent overconfidence)
- Requires a held-out calibration set

#### Isotonic Regression (Non-parametric)
- Fits a non-decreasing step function to map raw probabilities to calibrated ones
- More flexible than Platt scaling -- can capture non-linear miscalibration
- Requires more calibration data to avoid overfitting
- Recommended when miscalibration pattern is unknown

#### Temperature Scaling
- Divides logits by a learned temperature parameter T
- T > 1: softens probabilities (reduces overconfidence)
- T < 1: sharpens probabilities (increases confidence)
- Most commonly used for neural network calibration

#### Reconfidencing (2024 -- EMNLP Findings)
- Novel approach that trains a tree classifier to partition samples into sub-groups
- Calibrates at the sub-group level rather than marginally
- Uses Brier score decomposition (Calibration + Grouping Loss) for optimization
- Addresses the limitation that marginal calibration can hide sub-group errors

#### For LLM-Based Systems Specifically:
- **Verbalized confidence**: Ask the LLM to state its confidence (e.g., "I am 80% confident")
- **ConfTuner** (NeurIPS 2025): Fine-tunes LLMs with tokenized Brier score loss -- proven proper scoring rule
- **RLCR**: RL training with calibration rewards jointly improves accuracy and calibration
- Post-hoc calibration on LLM probability outputs using Platt scaling or isotonic regression

---

### 2.6 Recommended Evaluation Framework for KRA

| Metric | Purpose | Priority |
|--------|---------|----------|
| **Trifecta Exact Match** | Did we predict exact 1-2-3 order? | Primary |
| **Top-3 Set Match** | Did we predict the right 3 horses (any order)? | Primary |
| **NDCG@3** | Quality of ranking within top predictions | Primary |
| **Brier Score** | Overall prediction quality + calibration | High |
| **ECE** | Calibration of confidence estimates | High |
| **ROI (flat bet)** | Profitability with equal bet sizes | High |
| **Kelly ROI** | Profitability with optimal bet sizing | Medium |
| **Log Loss** | Penalizes overconfident wrong predictions | Medium |
| **Per-position accuracy** | Accuracy for 1st, 2nd, 3rd separately | Diagnostic |

---

## 3. Ensemble / Multi-Prompt Consensus Strategies

### 3.1 Self-Consistency with Chain of Thought (Wang et al., 2022)

**Description**: Instead of taking the single greedy output from CoT prompting, sample multiple diverse reasoning paths (using temperature > 0) and select the most frequent final answer via majority voting. The intuition: a correct answer is more likely to be reached through multiple valid reasoning paths.

**Quantitative Results**:
- Striking improvements across arithmetic, commonsense, and symbolic reasoning
- Typical improvement: +5% to +17% over standard CoT
- GSM8K improvements up to +17.9% with sufficient samples
- Works with any LLM (no training required)

**Implementation Complexity**: Low
- Just sample N responses with temperature > 0
- Count answer frequencies
- Select majority answer

**Key Formula for Error Reduction**:
P_ensemble = sum(C(k,i) * p^i * (1-p)^(k-i)) for i = ceil(k/2) to k
For p < 0.5 (individual error rate), ensemble error decreases exponentially with k.

**Limitations**:
- Requires answer extraction heuristic (only works for structured answers)
- Linear cost scaling with number of samples
- Cannot handle free-form answers
- Does not account for confidence differences between paths

**Source**: arXiv:2203.11171 | ICLR 2023

---

### 3.2 Universal Self-Consistency (USC) -- Google, 2024

**Description**: Extends self-consistency to free-form tasks where answer extraction is difficult. Instead of counting answer frequencies, USC concatenates all candidate responses and prompts the LLM itself to select the most consistent one.

**Process**:
1. Sample multiple responses with diverse reasoning paths
2. Concatenate all responses into a single prompt
3. Ask the LLM: "Given these N responses, which is the most consistent?"
4. LLM selects the winner

**Quantitative Results**:
- Matches standard self-consistency on math reasoning (without requiring answer extraction)
- Enables self-consistency for open-ended tasks (summarization, QA) where majority voting fails
- Particularly effective for code generation and long-form answers

**Implementation Complexity**: Low
- Simple prompt template with concatenated responses
- One additional LLM call for selection
- No training required

**Relevance to KRA**: Highly relevant because horse racing predictions may include reasoning about jockey, track conditions, and other factors that make answer extraction non-trivial. USC lets the LLM judge which prediction is most coherent.

**Limitations**:
- Scalability issues with many candidate responses (context window limits)
- LLM selection may have its own biases (position bias, verbosity bias)
- No confidence/certainty measure attached to the selection

**Source**: arXiv:2311.17311 | ICML 2024 Workshop

---

### 3.3 Confidence-Informed Self-Consistency (CISC, 2025)

**Description**: Enhances standard self-consistency by incorporating the LLM's self-assessed confidence into the voting process. Instead of equal-weight majority voting, each reasoning path's vote is weighted by a normalized confidence score.

**Process**:
1. Generate multiple reasoning paths (r_i, a_i)
2. For each path, extract self-assessed confidence c_i
3. Normalize: c_i_tilde = exp(c_i/T) / sum(exp(c_j/T)) (softmax with temperature T)
4. Weighted vote: select answer with highest total confidence-weighted support

**Quantitative Results**:
- Consistent improvements over unweighted self-consistency across 9 models and multiple benchmarks
- Improvements on GSM8K, Big-Bench-Hard, and MATH datasets
- Temperature parameter T provides useful control over confidence weighting

**Implementation Complexity**: Low-Medium
- Requires confidence extraction from LLM responses
- Simple weighted voting mechanism
- Temperature hyperparameter needs tuning

**Relevance to KRA**: Directly applicable -- weight predictions by model confidence. A high-confidence prediction of Horse A winning should count more than a low-confidence prediction of Horse B.

**Source**: ACL Findings 2025

---

### 3.4 Mixture-of-Agents (MoA) -- Together AI / ICLR 2025

**Description**: MoA uses a layered architecture where each layer contains multiple LLM "proposer" agents. Outputs from one layer become auxiliary input for the next layer's agents, who refine and aggregate the responses. A final "aggregator" agent produces the consolidated output.

**Architecture**:
```
Layer 1: [Agent A] [Agent B] [Agent C] [Agent D]  (proposers)
              |          |          |          |
Layer 2: [Agent E] [Agent F] [Agent G] [Agent H]  (refiners)
              |          |          |          |
Layer 3:           [Aggregator Agent]              (final output)
```

**Quantitative Results**:
- Together MoA (open-source models only) achieved 65.1% on AlpacaEval 2.0, surpassing GPT-4o (57.5%)
- Using GPT-4o as aggregator: score jumps from 74.1 to 83.3
- Most improvement comes from the first aggregation layer
- Model diversity (different architectures) is crucial for performance

**Best Practices**:
- Use strongest model as final aggregator
- Use diverse, heterogeneous models as proposers in earlier layers
- 2-3 layers typically sufficient (diminishing returns after)
- 4-6 agents per layer recommended

**Implementation Complexity**: Medium
- Requires access to multiple LLMs
- Higher latency (sequential layers)
- Higher cost (multiple LLM calls per query)
- Open-source implementation available

**Relevance to KRA**: Could use different prompting strategies (or temperature settings) as different "agents" rather than different models. Each agent analyzes the race from a different perspective (speed analysis, jockey analysis, track conditions, etc.), then an aggregator combines insights.

**Limitations**:
- High Time-to-First-Token (TTFT) due to sequential layers
- Cost scales multiplicatively with number of agents and layers
- Conformity risk: persuasive but incorrect agents can mislead the group
- Requires careful selection of diverse models/prompts

**Source**: arXiv:2406.04692 | ICLR 2025

---

### 3.5 MUSE (Multi-LLM Uncertainty via Subset Ensembles, 2025)

**Description**: An information-theoretic approach that uses Jensen-Shannon Divergence (JSD) to identify and aggregate well-calibrated subsets of LLMs. Rather than naively ensembling all models, MUSE selects a diverse yet coherent subset whose outputs are maximally informative.

**Two Selection Strategies**:
- **Greedy**: Start with most confident LLM, iteratively add models that increase diversity
- **Conservative**: More cautious selection prioritizing agreement

**Confidence Score**: c_i = |p_yes - 0.5| (distance from maximum uncertainty)

**Quantitative Results**:
- Improved calibration and predictive performance vs single-model and naive ensemble baselines
- Particularly effective for binary prediction tasks

**Relevance to KRA**: Could be adapted to select which prompt variants to trust for each specific race, based on the divergence of their predictions.

**Source**: EMNLP 2025

---

### 3.6 Iterative Consensus Ensemble (ICE, 2024)

**Description**: Multiple LLMs iteratively refine each other's reasoning across multiple rounds. In each round, models see their peers' previous answers and can revise their own. The process converges toward a stable consensus.

**Process**:
1. Each model generates initial prediction independently
2. Share all predictions across models
3. Each model revises its answer considering peers' reasoning
4. Repeat until consensus or max rounds

**Quantitative Results**:
- Up to 20%+ improvement over individual standalone models
- Matched performance of specialized reasoning models (O1-preview) on medical MCQs
- Improvements persist across multiple datasets (4,058 MCQs tested)
- Works even with simpler initial prompts

**Implementation Complexity**: Medium
- Requires orchestrating multiple LLM calls per round
- Multiple rounds of iteration add latency and cost
- Simple to implement conceptually

**Relevance to KRA**: Run multiple prediction passes, share predictions between rounds, let the model reconsider its analysis. Mimics a panel of racing experts debating.

**Source**: medRxiv 2024.12.25.24319629

---

### 3.7 PACER (Packet-Conditioned Revision, 2025)

**Description**: A lightweight coordination layer that generates a pool of reasoning traces, summarizes them into a compact "consensus packet" (top answers, support counts, short rationale per answer), and prompts each trace to self-review conditioned on this packet. Final prediction via confidence-weighted vote over revised answers.

**Three Stages**:
1. Generate pool of completed, screened traces
2. Summarize into consensus packet (low-bandwidth set-level evidence)
3. Each trace does brief self-review conditioned on packet, optionally revises answer

**Key Innovation**: Uses minimal coordination signal rather than full information sharing, making it more efficient than debate-style approaches.

**Implementation Complexity**: Medium
- Requires trace generation + summarization + revision pipeline
- Training-free, no additional verifier model needed

**Source**: arXiv:2602.02828 (2025)

---

### 3.8 Ranked Voting Self-Consistency (2025)

**Description**: Instead of single-answer majority voting, each LLM sample provides a ranked list of answers. Sophisticated voting methods (Borda count, instant-runoff, reciprocal rank) aggregate these ranked preferences.

**Relevance to KRA**: Directly applicable to trifecta prediction. Each reasoning path produces a ranking of horses, and ranked voting methods aggregate these rankings into a consensus ranking.

**Quantitative Results**: Improved robustness and accuracy over standard majority voting across multiple benchmarks and LLM families.

**Source**: ACL Findings 2025 (arXiv:2505.10772)

---

### 3.9 Comparison of Ensemble Strategies

| Method | Samples Needed | Extra Training | Handles Ranking | Cost Multiplier | Best For |
|--------|---------------|----------------|-----------------|-----------------|----------|
| Self-Consistency | 5-40 | No | No (majority vote) | Nx | Structured answers |
| USC | 3-10 | No | Yes (LLM selects) | N+1x | Free-form answers |
| CISC | 5-20 | No | Partial | Nx | Confidence-weighted |
| MoA | 4-24 | No | Yes | 3-6Nx (layers) | Multi-perspective |
| MUSE | 3-8 | Optional | No | 3-8x | Calibrated prediction |
| ICE | 3-5 models * rounds | No | Yes | 3-15x | Consensus building |
| PACER | 5-20 | No | Yes | N+2x | Efficient revision |
| Ranked Voting SC | 5-20 | No | Yes (native) | Nx | Ranking tasks |

---

## 4. Comparative Summary Table

### Best Methods for KRA Trifecta Prediction System

| Rank | Method | Category | Relevance to KRA | Expected Impact | Effort | Risk |
|------|--------|----------|-------------------|-----------------|--------|------|
| 1 | DSPy MIPROv2 | Prompt Optimization | High -- can optimize entire prediction pipeline | +13-20% accuracy | Medium | Low |
| 2 | Ranked Voting SC | Ensemble | Very High -- native ranking aggregation | +5-15% | Low | Low |
| 3 | PromptWizard | Prompt Optimization | High -- joint instruction + example optimization | SOTA gains | Medium | Low |
| 4 | EvoPrompt | Prompt Optimization | High -- simple, effective, gradient-free | Up to +25% | Low-Med | Low |
| 5 | CISC | Ensemble | High -- confidence-weighted ranking | +5-10% | Low | Low |
| 6 | Brier Score + ROI Eval | Evaluation | Critical -- calibration > accuracy for betting | +69% ROI | Low | None |
| 7 | MoA (Multi-Perspective) | Ensemble | Medium -- multi-angle analysis | Significant | Medium | Medium |
| 8 | TextGrad | Prompt Optimization | Medium -- good for complex reasoning prompts | +14% | Medium | Medium |
| 9 | ICE | Ensemble | Medium -- iterative refinement | +20% | Medium | Medium |
| 10 | Self-Refine | Prompt Optimization | Medium -- simple baseline | +5-20% | Low | Low |

---

## 5. Recommended Architecture for KRA System

Based on this research, the optimal automated prompt improvement architecture for the KRA prediction system would combine elements from multiple approaches:

### Phase 1: Offline Prompt Optimization Loop (Weekly/Daily)

```
[Seed Prompt] --> [PromptWizard or MIPROv2 Optimizer]
                         |
                    [Run on historical races]
                         |
                    [Evaluate with Brier Score + NDCG@3 + ROI]
                         |
                    [Error Classification]
                    (false positives vs false negatives,
                     by race type, track, conditions)
                         |
                    [Feedback-driven refinement]
                    (critique + synthesis cycle)
                         |
                    [Updated Prompt] --> repeat
```

**Recommended Optimizer**: PromptWizard (best balance of effectiveness vs API cost) or DSPy MIPROv2 (if willing to restructure as DSPy modules)

### Phase 2: Inference-Time Ensemble (Per Race)

```
[Race Data] --> [Prompt Variant 1] --> Prediction A (with confidence)
            --> [Prompt Variant 2] --> Prediction B (with confidence)
            --> [Prompt Variant 3] --> Prediction C (with confidence)
            --> [Prompt Variant 4] --> Prediction D (with confidence)
            --> [Prompt Variant 5] --> Prediction E (with confidence)
                        |
                [Ranked Voting Aggregation with CISC]
                (confidence-weighted Borda count)
                        |
                [Calibration Layer]
                (isotonic regression on held-out data)
                        |
                [Final Trifecta Prediction + Calibrated Confidence]
```

**Recommended Approach**: 5 diverse prompts (different analytical perspectives) with Ranked Voting + Confidence-Informed aggregation

### Phase 3: Evaluation Pipeline

```
[Predictions] vs [Actual Results]
        |
    [Compute Metrics]
    - Brier Score (overall quality)
    - NDCG@3 (ranking quality)
    - Top-3 Set Match (basic accuracy)
    - ECE (calibration quality)
    - ROI (flat bet profitability)
        |
    [Error Analysis]
    - By race type (sprint/distance)
    - By track condition (fast/muddy)
    - By field size
    - By odds distribution
        |
    [Feed errors back to Phase 1 optimizer]
```

### Implementation Priority

1. **Immediate** (Low effort, high impact): Add Brier Score and NDCG@3 metrics to existing evaluation. Add calibration layer (isotonic regression). Implement Self-Consistency (5 samples with majority voting).

2. **Short-term** (Medium effort): Implement Ranked Voting SC with confidence weighting. Create diverse prompt variants (speed-focused, form-focused, conditions-focused). Add EvoPrompt optimization loop for prompt evolution.

3. **Medium-term** (Higher effort): Migrate to DSPy or PromptWizard for systematic prompt optimization. Implement full MoA-style multi-perspective analysis. Build error classification and feedback loop for continuous improvement.

---

## 6. Sources

### Prompt Optimization Frameworks
- DSPy MIPROv2: https://dspy.ai | Opsahl-Ong et al., 2024
- TextGrad: arXiv:2406.07496 | https://github.com/zou-group/textgrad (Nature 2025)
- OPRO: arXiv:2309.03409 | Yang et al., Google DeepMind (ICLR 2024)
- EvoPrompt: arXiv:2309.08532 | Guo et al. (ICLR 2024)
- APE: arXiv:2211.01910 | Zhou et al., 2022
- PE2: ACL Findings 2024 | Ye et al., 2023
- PromptBreeder: arXiv:2309.16797 | Fernando et al. (NeurIPS 2023)
- PromptWizard: arXiv:2405.18369 | Microsoft Research (ACL Findings 2025)
- Self-Refine: arXiv:2303.17651 | Madaan et al., 2023
- metaTextGrad: arXiv:2505.18524 | 2025
- GReaTer: OpenReview, ICLR 2025
- SPO (Self-Supervised Prompt Optimization): EMNLP Findings 2025

### Evaluation Metrics
- Brier Score in LLM Forecasting: arXiv:2402.18563 | Halawi et al., 2024
- LLM-as-a-Prophet: arXiv:2510.17638
- ConfTuner: NeurIPS 2025 | Li et al.
- RLCR: OpenReview, 2025
- Reconfidencing: EMNLP Findings 2024
- Calibration vs Accuracy for Betting: Walsh & Joshi, Machine Learning with Applications 16, 2024
- Prophet Arena Scoring: https://ai-prophet.github.io/pm_ranking/
- Horse Racing ML Survey: Consensus.app aggregation (2024)
- Learning-to-Rank for Horse Racing: Korean Journal of Applied Statistics, 2024

### Ensemble Methods
- Self-Consistency: arXiv:2203.11171 | Wang et al. (ICLR 2023)
- Universal Self-Consistency: arXiv:2311.17311 | Chen et al. (ICML 2024)
- CISC: ACL Findings 2025
- Mixture-of-Agents: arXiv:2406.04692 | Wang et al. (ICLR 2025)
- MUSE: EMNLP 2025
- ICE: medRxiv 2024.12.25.24319629
- PACER: arXiv:2602.02828 (2025)
- Ranked Voting SC: arXiv:2505.10772 (ACL Findings 2025)
- Deep Think with Confidence: arXiv:2508.15260
- Awesome-LLM-Ensemble: https://github.com/junchenzhi/Awesome-LLM-Ensemble
- LENS: arXiv:2507.23167
- LLM Ensemble for Categorization: arXiv:2511.15714
